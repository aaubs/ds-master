{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundations of Supervised Machine Learning for Business Analytics\n",
        "## A Practical Introduction to Predictive Modeling\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this introduction to Supervised Machine Learning (SML). This notebook is designed for Master's students in Business Data Science, providing a foundational understanding of predictive modeling through a practical, business-oriented lens. We will bridge the gap between statistical theory and its application to solve tangible business problems.\n",
        "\n",
        "The core idea of supervised learning is to learn a mapping function from input variables (`X`) to an output variable (`Y`). This is formally expressed as:\n",
        "\n",
        "$$ Y = f(X) + \\epsilon $$\n",
        "\n",
        "Where `f` is the unknown function we aim to estimate, and `Œµ` represents the irreducible error. Our goal is to find an estimated function, `fÃÇ`, that accurately predicts `Y` for new, unseen data `X`.\n",
        "\n",
        "**üéØ Learning Objectives:**\n",
        "\n",
        "By the end of this session (approx. 60-75 minutes), you will be able to:\n",
        "\n",
        "1.  **Formulate** business problems as either regression or classification tasks.\n",
        "2.  **Explain** the principle of out-of-sample validation and its role in preventing overfitting.\n",
        "3.  **Implement** and interpret two fundamental models‚ÄîLinear Regression and Logistic Regression‚Äîusing `scikit-learn`.\n",
        "4.  **Introduce** the Bias-Variance Tradeoff as a central concept in model complexity.\n",
        "5.  **Evaluate** model performance using business-relevant metrics (RMSE, Precision, Recall) and diagnostic tools (Confusion Matrix).\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Session Setup: Importing Libraries\n",
        "\n",
        "First, we load the necessary Python libraries. We rely on `scikit-learn` for modeling, `pandas` for data management, and specialized libraries like `mlxtend` for enhanced visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn for modeling and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix\n",
        "\n",
        "# Mlxtend for enhanced visualizations\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# Set a professional plot style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Regression: Predicting Continuous Outcomes\n",
        "\n",
        "We begin with regression, a class of SML problems where the objective is to predict a continuous numerical value.\n",
        "\n",
        "**üéØ Section Objectives:**\n",
        "\n",
        "-   Translate a business forecasting question into a regression model.\n",
        "-   Implement a training and testing split to ensure model generalization.\n",
        "-   Build, train, and interpret a linear regression model.\n",
        "-   Quantify model error in financial terms.\n",
        "\n",
        "### 1.1. The Business Problem: Forecasting E-commerce Revenue\n",
        "\n",
        "Imagine you are a data scientist for a Danish e-commerce company specializing in designer furniture. To optimize inventory and marketing budgets for the upcoming quarter, management needs a reliable forecast of monthly sales based on planned digital advertising spend.\n",
        "\n",
        "### 1.2. Data Simulation\n",
        "\n",
        "We will simulate data reflecting this scenario. Let's assume a baseline monthly sales of 250,000 DKK, with an additional 3,500 DKK in sales for every 1,000 DKK spent on advertising. This relationship is subject to random market fluctuations (the `Œµ` term)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a RandomState for reproducibility, ensuring the \"random\" data is the same every time\n",
        "rng = np.random.RandomState(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate 200 months of data\n",
        "n_samples = 200\n",
        "ad_spend_kdkk = rng.uniform(low=10, high=100, size=n_samples)\n",
        "market_noise = rng.normal(loc=0, scale=15, size=n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The \"true\" underlying data generating process\n",
        "# Sales are in thousands of DKK (kDKK)\n",
        "sales_kdkk = 250 + (3.5 * ad_spend_kdkk) + market_noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assemble into a pandas DataFrame for structured analysis\n",
        "sales_df = pd.DataFrame({\n",
        "    'ad_spend_kdkk': ad_spend_kdkk,\n",
        "    'sales_kdkk': sales_kdkk\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the first few rows of our dataset\n",
        "print(\"Simulated Dataset Head:\")\n",
        "sales_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Before modeling, visualizing the data is crucial to validate our assumptions. A scatter plot should reveal the linear relationship we expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(data=sales_df, x='ad_spend_kdkk', y='sales_kdkk', alpha=0.8)\n",
        "plt.title(\"Monthly Sales vs. Advertising Spend\")\n",
        "plt.xlabel(\"Advertising Spend (kDKK)\")\n",
        "plt.ylabel(\"Monthly Sales (kDKK)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot confirms a clear, positive linear trend, making linear regression a suitable candidate model.\n",
        "\n",
        "### 1.4. The Principle of Out-of-Sample Validation\n",
        "\n",
        "A model's true value is its ability to generalize to new, unseen data. To measure this, we partition our data:\n",
        "\n",
        "-   **Training Set:** Used to \"teach\" the model. The model learns the parameters by observing this data.\n",
        "-   **Testing Set:** Held back during training. We use this to get an unbiased estimate of the model's performance on future data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the feature matrix (X) and target vector (y)\n",
        "# X must be a 2D array-like object (e.g., a DataFrame)\n",
        "X = sales_df[['ad_spend_kdkk']]\n",
        "y = sales_df['sales_kdkk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partition the data using an 80/20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the shapes of our new datasets\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size:  {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5. Model Training\n",
        "\n",
        "We now instantiate a `LinearRegression` model from `scikit-learn` and fit it to our training data. The `.fit()` method is where the model estimates the optimal coefficients (`Œ≤ÃÇ‚ÇÄ` and `Œ≤ÃÇ‚ÇÅ`) that minimize the sum of squared residuals for the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Instantiate the model object\n",
        "sales_model = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Fit the model to the training data\n",
        "sales_model.fit(X_train, y_train)\n",
        "print(\"Model training is complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6. Model Interpretation\n",
        "\n",
        "A key advantage of linear models is their interpretability. We can directly inspect the learned coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the learned intercept (Œ≤ÃÇ‚ÇÄ) and coefficient (Œ≤ÃÇ‚ÇÅ)\n",
        "intercept = sales_model.intercept_\n",
        "coefficient = sales_model.coef_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Learned Intercept (Œ≤ÃÇ‚ÇÄ): {intercept:.2f}\")\n",
        "print(f\"Learned Coefficient for Ad Spend (Œ≤ÃÇ‚ÇÅ): {coefficient:.2f}\")\n",
        "print(f\"\\nBusiness Interpretation: The model suggests a baseline sales of {intercept:,.2f} kDKK, with each additional 1,000 DKK in ad spend contributing to a {coefficient:,.2f} kDKK increase in sales.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model has successfully approximated the true parameters (`Œ≤‚ÇÄ`=250, `Œ≤‚ÇÅ`=3.5) we defined during data simulation.\n",
        "\n",
        "### 1.7. Model Evaluation\n",
        "\n",
        "Now, we use the trained model to make predictions on `X_test`. For regression, the Root Mean Squared Error (RMSE) is a standard metric that measures the typical size of the prediction errors, in the same units as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on the unseen test set\n",
        "y_pred = sales_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Root Mean Squared Error (RMSE) on Test Data: {rmse:,.2f} kDKK\")\n",
        "print(f\"\\nBusiness Interpretation: On average, our model's monthly sales predictions are off by approximately {rmse*1000:,.0f} DKK. This error margin must be assessed for its acceptability in business planning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we visualize the model's performance by plotting its prediction line against the actual test data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "# Plot the actual data points from the test set\n",
        "sns.scatterplot(x=X_test['ad_spend_kdkk'], y=y_test, label='Actual Sales (Test Set)', s=100, color='blue')\n",
        "# Plot the model's prediction line\n",
        "plt.plot(X_test['ad_spend_kdkk'], y_pred, color='red', linewidth=3, label='Predicted Sales (Model)')\n",
        "plt.title(\"Model Performance on Unseen Test Data\")\n",
        "plt.xlabel(\"Advertising Spend (kDKK)\")\n",
        "plt.ylabel(\"Monthly Sales (kDKK)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.8. The Bias-Variance Tradeoff\n",
        "\n",
        "Our linear model is simple, but is it *too* simple? This introduces the central challenge in machine learning: the **bias-variance tradeoff**.\n",
        "\n",
        "-   **Bias:** Error from erroneous assumptions. High bias can cause a model to miss relevant relations (underfitting).\n",
        "-   **Variance:** Error from sensitivity to small fluctuations in the training data. High variance can cause a model to fit the noise instead of the signal (overfitting).\n",
        "\n",
        "<a href=\"https://imgflip.com/i/a6im0n\"><img src=\"https://i.imgflip.com/a6im0n.jpg\" title=\"made at imgflip.com\" width=\"400\"/></a>\n",
        "\n",
        "Splitting data into training and testing sets is our primary tool for diagnosing this tradeoff. If training error is low but testing error is high, we have a high-variance problem (overfitting).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Classification: Predicting Categorical Outcomes\n",
        "\n",
        "We now turn to classification, where the goal is to predict a discrete class label.\n",
        "\n",
        "**üéØ Section Objectives:**\n",
        "\n",
        "-   Frame a customer retention problem as a classification task.\n",
        "-   Build and train a logistic regression model.\n",
        "-   Differentiate between class and probability predictions.\n",
        "-   Evaluate performance using a confusion matrix, precision, and recall.\n",
        "\n",
        "### 2.1. The Business Problem: Proactive Churn Prevention\n",
        "\n",
        "You are a data scientist for a rapidly growing Copenhagen-based tech company with a creator-centric, premium content platform. Users subscribe for exclusive monthly access to content from their favorite creators, engaging via direct messaging and tipping features. Success hinges on strong user-creator relationships.\n",
        "\n",
        "The key challenge is user churn. A user might cancel if their preferred creator becomes inactive or if they are dissatisfied with the content. Your goal is to build a model that predicts churn based on user satisfaction scores, enabling a 'Proactive Retention Team' to offer targeted incentives to at-risk users.\n",
        "\n",
        "### 2.2. Data Simulation\n",
        "\n",
        "We simulate data for 500 customers, where a lower satisfaction score significantly increases the probability of churn. This non-linear relationship is well-modeled by the logistic (sigmoid) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(10)\n",
        "n_customers = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate customer satisfaction scores (normally distributed around 3.0)\n",
        "satisfaction_score = rng.normal(loc=3.0, scale=0.8, size=n_customers)\n",
        "# Constrain scores to a realistic 1-5 range\n",
        "satisfaction_score = np.clip(satisfaction_score, 1, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The sigmoid function transforms a linear relationship into a probability\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Churn probability is sensitive to scores below a threshold (e.g., 2.5)\n",
        "churn_probability = sigmoid(-2 * (satisfaction_score - 2.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate churn outcome (0 = Stayed, 1 = Churned) based on this probability\n",
        "churn = rng.binomial(1, churn_probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assemble the DataFrame\n",
        "churn_df = pd.DataFrame({\n",
        "    'satisfaction_score': satisfaction_score,\n",
        "    'churn': churn\n",
        "})\n",
        "\n",
        "print(\"Simulated Customer Churn Dataset Head:\")\n",
        "churn_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "A `regplot` with `logistic=True` helps visualize the S-shaped relationship between our feature and the probability of the outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "sns.regplot(data=churn_df, x='satisfaction_score', y='churn', logistic=True, ci=None,\n",
        "            scatter_kws={'alpha': 0.2, 'color': 'purple'}, line_kws={'color': 'black'})\n",
        "plt.title(\"Customer Churn Probability vs. Satisfaction Score\")\n",
        "plt.xlabel(\"Satisfaction Score (1-5)\")\n",
        "plt.ylabel(\"Churn (1 = Churned, 0 = Stayed)\")\n",
        "plt.yticks([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Model Training\n",
        "\n",
        "The process mirrors regression: we split, instantiate, and fit. Here, we use `LogisticRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature and target\n",
        "X = churn_df[['satisfaction_score']]\n",
        "y = churn_df['churn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data, stratifying by y to maintain churn proportion in both sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate and fit the model\n",
        "churn_model = LogisticRegression(random_state=42)\n",
        "churn_model.fit(X_train, y_train)\n",
        "print(\"Churn prediction model training is complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5. Class vs. Probability Predictions\n",
        "\n",
        "Classification models can provide a final class prediction (0 or 1) or the underlying probability estimate. Probabilities are often more valuable for business decisions, allowing for threshold-based strategies (e.g., only contact customers with >80% churn probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the final class (0 or 1), using a default 0.5 probability threshold\n",
        "y_pred_class = churn_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict the probability for each class. We are interested in the probability of the positive class (Churn=1)\n",
        "y_pred_proba = churn_model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame to compare actuals with predictions\n",
        "results_df = X_test.copy().reset_index(drop=True)\n",
        "results_df['actual_churn'] = y_test.reset_index(drop=True)\n",
        "results_df['predicted_churn_class'] = y_pred_class\n",
        "results_df['predicted_churn_probability'] = y_pred_proba\n",
        "\n",
        "print(\"Model Predictions on Test Customers:\")\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.6. Model Evaluation: The Confusion Matrix\n",
        "\n",
        "The **Confusion Matrix** provides a detailed breakdown of a classifier's performance, which is far more insightful than a single accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we compute the confusion matrix using scikit-learn\n",
        "cm = confusion_matrix(y_test, y_pred_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, we use mlxtend to plot this matrix for a clearer visualization\n",
        "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True,\n",
        "                                figsize=(7, 7),\n",
        "                                class_names=['Stayed', 'Churned'])\n",
        "plt.title(\"Churn Model Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Matrix Components:**\n",
        "-   **True Negatives (TN):** Top-Left. Correctly predicted 'Stayed'.\n",
        "-   **False Positives (FP):** Top-Right. Incorrectly predicted 'Churned'. (Type I Error)\n",
        "-   **False Negatives (FN):** Bottom-Left. Incorrectly predicted 'Stayed'. (Type II Error)\n",
        "-   **True Positives (TP):** Bottom-Right. Correctly predicted 'Churned'.\n",
        "\n",
        "### 2.7. Precision and Recall\n",
        "\n",
        "From the confusion matrix, we derive more nuanced metrics. The choice between optimizing for Precision or Recall is always dictated by business context.\n",
        "\n",
        "-   **Precision:** Of all the instances the model predicted as positive, how many were actually positive? (Focus on the cost of a False Positive)\n",
        "    $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
        "-   **Recall (Sensitivity):** Of all the actual positive instances, how many did the model correctly identify? (Focus on the cost of a False Negative)\n",
        "    $$ \\text{Recall} = \\frac{TP}{TP + FN} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The classification_report provides a concise summary of these metrics\n",
        "print(classification_report(y_test, y_pred_class, target_names=['Stayed (0)', 'Churned (1)']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "-   **Precision (for 'Churned'):** Our model has 85% precision. When it predicts a customer will churn, it is correct 85% of the time. This ensures retention efforts are well-targeted.\n",
        "-   **Recall (for 'Churned'):** Our model has 73% recall. It successfully identifies 73% of all customers who *actually* churned. This means we are failing to identify 27% of at-risk customers‚Äîa significant opportunity cost.\n",
        "\n",
        "ü§î **Discussion Question:** The retention team has a limited budget. Should they ask you to optimize the model for higher Precision or higher Recall? What are the business implications of each choice?\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Conclusion and Future Directions\n",
        "\n",
        "In this session, we have explored the foundational workflow of supervised machine learning, from defining a business problem to simulating data, training a model, and, most critically, evaluating its performance in a business context.\n",
        "\n",
        "**Core Concepts Revisited:**\n",
        "1.  **Problem Formulation:** SML problems are broadly categorized into regression (predicting quantities) and classification (predicting labels).\n",
        "2.  **Generalization:** The train-test split is a non-negotiable step to estimate a model's performance on unseen data.\n",
        "3.  **Model Selection:** We used simple, interpretable models as a starting point. Model choice is a function of the problem type, data complexity, and the need for interpretability.\n",
        "4.  **Contextual Evaluation:** Performance metrics are only meaningful when interpreted through the lens of business objectives and the costs associated with different types of model error.\n",
        "\n",
        "**Future Directions:**\n",
        "-   **Feature Engineering:** Real-world performance is often driven by the quality of input features.\n",
        "-   **Model Complexity:** Explore more complex models like Decision Trees, Random Forests, and Gradient Boosted Machines.\n",
        "-   **Regularization:** Techniques like Ridge (L2) and Lasso (L1) regression prevent overfitting by penalizing model complexity.\n",
        "-   **Hyperparameter Tuning:** Systematically tuning model settings can significantly improve performance.\n",
        "\n",
        "This notebook serves as the first step. The principles of data partitioning, model fitting, and critical evaluation are universal and will apply as you progress to more advanced techniques."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langcorn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
