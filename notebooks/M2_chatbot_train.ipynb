{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M2_chatbot_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyxDgeviYAWi"
      },
      "source": [
        "# Simple FAQ chatbot\n",
        "\n",
        "![](https://source.unsplash.com/V5vqWC9gyEU)\n",
        "\n",
        "This is a variation of https://github.com/python-engineer/pytorch-chatbot, and a toy-example.\n",
        "For a more complex example check out (e.g.) https://github.com/MrJay10/banking-faq-bot. Datacamp has some courses on that topic, too.\n",
        "\n",
        "The data needed for building such a system is a collection from a company FAQ for instance with variations for each question-answer pair type. For examples with many q-a pairs like the banking-faq, you can consider similarity-based approaches, where input text is matched with most \"similar\" predifined questions and then the user picks upon request - \"did you mean xyz-question\"\n",
        "Alternatively, one could create paraphrased versions of questions for each question-answer pair to have more training examples. This can be done manually or \n",
        "\n",
        "The overall architecture is as follows:\n",
        "\n",
        "*   Given a free text/prompt, predict which question is asked (intent)\n",
        "*   Pick corresponding answer (e.g. random out of 2-3) to simulate dialogue\n",
        "\n",
        "In this notebook, we will first use TFIDF-Logit, then standard SpaCy vectors and finally Floret (new SpaCy vectors that combine a new efficient compression with fastText, that helps overcome typos by including subword-elements in the model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O18yPl__dtpc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UG0NwwC9JS9z"
      },
      "outputs": [],
      "source": [
        "# utils...\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nth-mmRSaAue"
      },
      "source": [
        "we will be using the medium-sized spacy model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUQklcMMOAqa",
        "outputId": "d0808c0e-a633-4bec-89db-a8276e4c7e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-01 13:50:30.589015: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.8 MB 66.2 MB/s \n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "! spacy download en_core_web_md --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ocLVwJT6eJfy"
      },
      "outputs": [],
      "source": [
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_md') #instantiating English module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XoLr_SehePR0"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJzNOJjKaF6p"
      },
      "source": [
        "we use spacy for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cc87yUzfflkc"
      },
      "outputs": [],
      "source": [
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a list/iterable of texts\n",
        "  removes twitter stuff\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  for text in nlp.pipe(texts, disable=[\"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text # lemmatize and lower\n",
        "          if token.is_alpha # remove numbers\n",
        "          and not token.is_punct] # remove punctoation\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "  \n",
        "  return clean_container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqR6QJ99aL9O"
      },
      "source": [
        "The data is a json-file with questions and answers as well as \"intent-tags\"\n",
        "We can open it from local or grab it from remote with requests (only one option).\n",
        "The reasonn for using a json here is that variations of questions and answers are independent (number/linking; n-n). e.g. you can have 2 ways to ask and 4 ways to answer for a specific issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afc5iEBTf7vA"
      },
      "outputs": [],
      "source": [
        "#open loacal file\n",
        "# data = json.load(open('chatbot-convo.json','r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "avU96skeH818"
      },
      "outputs": [],
      "source": [
        "# stream file from remote online\n",
        "r = requests.get('https://github.com/aaubs/ds-master/raw/main/data/chatbot-convo.json')\n",
        "data = json.loads(r.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od_NcosWayl1"
      },
      "source": [
        "For training, we need to get all questions and associated labels into tabular format. We iterate over all intents in the data and zip them with the intent-label into a list of tupels. Zip is a great function to bind 2 lists into a list of tupels `[a,b,c], [1,2,3] â†’ [(a,1), (b,2), (c,3)]`\n",
        "we multiply the label `l*[i['tag']]` to reach `len(i['patterns']) == len(l*[i['tag']])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nWaCHVx_gdyC"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "\n",
        "for i in data['intents']:\n",
        "  l = len(i['patterns'])\n",
        "  tuples = list(zip(i['patterns'], l*[i['tag']])) \n",
        "  training.extend(tuples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O7bGagjbgenf"
      },
      "outputs": [],
      "source": [
        "training_df = pd.DataFrame(training, columns=['txt','label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccuz4f8Zf-u0"
      },
      "outputs": [],
      "source": [
        "training_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoisfhHSiBS1",
        "outputId": "906340d7-fa3c-4aa7-ffec-07d1eb25edb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "training_df.txt_p = text_prepro(training_df.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6ogumcNqiKDM"
      },
      "outputs": [],
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjbAC947ilWU",
        "outputId": "efc496ee-30f0-4695-eeb6-44568d19d006"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
              "                ('logisticregression', LogisticRegression())])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe.fit(training_df.txt_p, training_df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHK4GHj2jJL6",
        "outputId": "706a6200-fd9f-4e93-b8f0-aa7c3bb748cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['funny'], dtype=object)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe.predict(['tell me something funny'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TU6B3CZbk0Uj"
      },
      "outputs": [],
      "source": [
        "# let's give our bot a name\n",
        "bot_name = 'ðŸ’¬ Hugo'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34db35tbpio"
      },
      "source": [
        "Input triggers reply, which preprocesses the text, asks the model for a prediction (which class of question?), then iterates over all possible intent-types and where it finds a match, it retrieves a random response from this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8FyKHuP2jv_7"
      },
      "outputs": [],
      "source": [
        "def reply(txt):\n",
        "  clean_text = text_prepro([txt])\n",
        "  tag = pipe.predict(clean_text)[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD4vca4slXxL",
        "outputId": "8ae6c4e8-5454-41db-ccbd-586338217164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ Hugo: Why did the hipster burn his mouth? He drank the coffee before it was cool.\n"
          ]
        }
      ],
      "source": [
        "reply('Tell me a joke')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HNti4NsJZFW"
      },
      "source": [
        "### Using SpaCy for vectorization\n",
        "Using \"traditional\" spacy embeddings. Medium web model.\n",
        "When using spacy, we rely on pretrained vectors i.e. spacy takes care of all preprocessing and vectorization internally. Thus, we can skip TFIDF and go directly to model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_1E9nP1LIi0z"
      },
      "outputs": [],
      "source": [
        "# we use again logistic regression\n",
        "model_spacy = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "fdZdevGKJr4L",
        "outputId": "d92aaa0d-bab5-4d2e-d014-1a36419024f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b5acdaed-08f8-4952-975e-b8a34b66371a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>txt</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hey</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How are you</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is anyone there?</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Good day</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bye</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>See you later</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Goodbye</td>\n",
              "      <td>goodbye</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Thanks</td>\n",
              "      <td>thanks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5acdaed-08f8-4952-975e-b8a34b66371a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5acdaed-08f8-4952-975e-b8a34b66371a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5acdaed-08f8-4952-975e-b8a34b66371a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                txt     label\n",
              "0                Hi  greeting\n",
              "1               Hey  greeting\n",
              "2       How are you  greeting\n",
              "3  Is anyone there?  greeting\n",
              "4             Hello  greeting\n",
              "5          Good day  greeting\n",
              "6               Bye   goodbye\n",
              "7     See you later   goodbye\n",
              "8           Goodbye   goodbye\n",
              "9            Thanks    thanks"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BxKLgpKzJzLr"
      },
      "outputs": [],
      "source": [
        "# we grab the vectors for all texts and stack them into a matrix\n",
        "X_train = np.vstack([txt.vector for txt in nlp.pipe(training_df.txt, disable=[\"parser\", \"ner\"])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1EcQ14iKwMn"
      },
      "outputs": [],
      "source": [
        "# quick explaininng of the vectors (not really part of the code)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEjzytCVKUhn",
        "outputId": "71051f44-8308-4db3-86ea-30f1af0bc429"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# training\n",
        "model_spacy.fit(X_train, training_df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xeJB2B94Kd9s"
      },
      "outputs": [],
      "source": [
        "bot_name = 'ðŸ’¬ SpaCy Hugo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jy3uPsUyMLYd"
      },
      "outputs": [],
      "source": [
        "def reply_spacy(txt):\n",
        "  tag = model_spacy.predict([nlp(txt).vector])[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0j-ew5kMXga",
        "outputId": "d07adb31-eed4-437f-899e-f57d2e7bcfcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ SpaCy Hugo: We accept VISA, Mastercard and Paypal\n"
          ]
        }
      ],
      "source": [
        "reply_spacy('how can I pay?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB3KPlztOqEQ"
      },
      "source": [
        "### Trying out Floret ðŸŒ¸\n",
        "In 2017 Facebook introduced [fastText](https://fasttext.cc/) that includes sub-word elements in the model creation. In April 2022 Explorion.ai presented Blom Embeddings - an elegant way to reduce embessing size. Later In August 2022 they introduced [floret](https://explosion.ai/blog/floret-vectors), an approach to jon fastText with Bloom (not related to https://huggingface.co/bigscience/bloom)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE40R9dNOXYi",
        "outputId": "ae8ec706-65a0-4197-dfab-9037669a4b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |â–ˆ                               | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆ                              | 20 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆ                             | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–                           | 40 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316 kB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# get floret installed\n",
        "! python -m pip install floret 'spacy~=3.4.0' --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYu84PJHRyor",
        "outputId": "3d78308c-dc6f-4680-b845-e6bab3f24088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-01 14:22:55--  https://github.com/explosion/spacy-vectors-builder/releases/download/en-3.4.0/en_vectors_floret_md.floret.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/468670482/dea3b10f-1c47-47f0-841e-542650ab9d80?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221101T142256Z&X-Amz-Expires=300&X-Amz-Signature=d9d8a4b5826f71e6ced283978140bb98ede161a80d40bfdd0d55ca9e4f86e868&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=468670482&response-content-disposition=attachment%3B%20filename%3Den_vectors_floret_md.floret.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-11-01 14:22:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/468670482/dea3b10f-1c47-47f0-841e-542650ab9d80?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221101T142256Z&X-Amz-Expires=300&X-Amz-Signature=d9d8a4b5826f71e6ced283978140bb98ede161a80d40bfdd0d55ca9e4f86e868&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=468670482&response-content-disposition=attachment%3B%20filename%3Den_vectors_floret_md.floret.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47586560 (45M) [application/octet-stream]\n",
            "Saving to: â€˜en_vectors_floret_md.floret.gzâ€™\n",
            "\n",
            "en_vectors_floret_m 100%[===================>]  45.38M   167MB/s    in 0.3s    \n",
            "\n",
            "2022-11-01 14:22:56 (167 MB/s) - â€˜en_vectors_floret_md.floret.gzâ€™ saved [47586560/47586560]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download floret vectors\n",
        "! wget -nc https://github.com/explosion/spacy-vectors-builder/releases/download/en-3.4.0/en_vectors_floret_md.floret.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOCS-EeQR3vd",
        "outputId": "18c442a1-1fa7-4da4-8973-5dc584a6830d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-01 14:23:41.056906: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;4mâ„¹ Creating blank nlp object for language 'en'\u001b[0m\n",
            "[2022-11-01 14:23:42,080] [INFO] Reading vectors from en_vectors_floret_md.floret.gz\n",
            "INFO:spacy:Reading vectors from en_vectors_floret_md.floret.gz\n",
            "50000it [00:03, 13114.75it/s]\n",
            "[2022-11-01 14:23:45,982] [INFO] Loaded vectors from en_vectors_floret_md.floret.gz\n",
            "INFO:spacy:Loaded vectors from en_vectors_floret_md.floret.gz\n",
            "\u001b[38;5;2mâœ” Successfully converted 50000 vectors\u001b[0m\n",
            "\u001b[38;5;2mâœ” Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/en_vectors_floret_md\n"
          ]
        }
      ],
      "source": [
        "# spacy initialize them internally\n",
        "! spacy init vectors en en_vectors_floret_md.floret.gz en_vectors_floret_md --mode floret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5tPCNauPR7FK"
      },
      "outputs": [],
      "source": [
        "# This is the spaCy pipeline with floret vectors\n",
        "nlp_fl = spacy.load(\"en_vectors_floret_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps8-nijNc07o"
      },
      "source": [
        "how are these vectors different from standard spacy?\n",
        "the difference lies in OOV (out of vocabulary) words. Such words can occur where there are jargon or typos. Standard vecs in spacy contain a many 10k vocabularies. However, not everything can be covered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCMEd3ncSHm9",
        "outputId": "601bcd5c-09e4-4764-82bd-f374ee3ab1a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_1 = nlp.vocab[\"univercities\"]\n",
        "word_2 = nlp.vocab[\"universities\"]\n",
        "\n",
        "word_1.similarity(word_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZc2MPnHSOmS",
        "outputId": "1659ac3f-cc9a-4aa8-9277-6aeadf0951ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# typo --> 0-vector\n",
        "word_1.vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngU7v7wNdPED"
      },
      "source": [
        "Floret vectors will also rely on \n",
        "`<univ', 'unive', 'niver', 'iverc', 'verci', 'ercit', 'rciti', 'citie', 'ities', 'ties>` and get it closer to right..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kikMFacVSa0J",
        "outputId": "b3ef8305-b253-4a1b-9819-e03109764306"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7986329197883606"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_1 = nlp_fl.vocab[\"univercities\"]\n",
        "word_2 = nlp_fl.vocab[\"universities\"]\n",
        "\n",
        "word_1.similarity(word_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9yY71axSg01"
      },
      "outputs": [],
      "source": [
        "word_1.vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CsBLufH5SvUC"
      },
      "outputs": [],
      "source": [
        "X_train = np.vstack([txt.vector for txt in nlp_fl.pipe(training_df.txt, disable=[\"parser\", \"ner\"])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "3BecPqZ3S8IB"
      },
      "outputs": [],
      "source": [
        "model_fl = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRTVEuVcTBKu",
        "outputId": "4e53de92-8e1e-4c6c-8461-e1d96988669a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_fl.fit(X_train, training_df.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "RQSxFZnNTR6Y"
      },
      "outputs": [],
      "source": [
        "bot_name = 'ðŸŒ¸ Floret Jeppe'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5QmzneFaTR6Z"
      },
      "outputs": [],
      "source": [
        "def reply_fl(txt):\n",
        "  tag = model_fl.predict([nlp_fl(txt).vector])[0]\n",
        "  for intent in data['intents']:\n",
        "    if tag == intent[\"tag\"]:\n",
        "      print(f\"{bot_name}: {random.choice(intent['responses'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z29jE39ATR6Z",
        "outputId": "e05c7d73-8246-4f2a-e3cc-6dc08fee84c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŒ¸ Floret Jeppe: Delivery takes 2-4 days\n"
          ]
        }
      ],
      "source": [
        "reply_fl('which day does my stuff arrive?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5V_zPG6XxB"
      },
      "source": [
        "## Run Chatbot command-line style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "DKH3OiPK6SBu",
        "outputId": "97af48bf-1d5b-498e-f8af-9521a045f14f"
      },
      "outputs": [],
      "source": [
        "bot_name = \"ðŸŒ¸ Floret Hugo\"\n",
        "your_name = \"ðŸŽƒ RJ\"\n",
        "\n",
        "\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "    sentence = input(f\"{your_name}: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "    reply_fl(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bot_name = \"ðŸŒ¸ Floret Hugo\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        # use the reply_fl function to get the bot's message\n",
        "        bot_message = reply_fl(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        time.sleep(2)\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcxerG08MCh"
      },
      "source": [
        "### Chatbot app with streamlit:\n",
        "\n",
        "There is an extension which allows to implement a chatbot in ST:\n",
        "https://ai-yash-st-chat-exampleschatbot-fkuecs.streamlitapp.com/\n",
        "\n",
        "Example code can be found here:\n",
        "https://github.com/AI-Yash/st-chat/blob/main/examples/chatbot.py\n",
        "\n",
        "The bot in the example runs a rather fancy online deployed model by FB that actually evaluates the whole dialogue rather than individual sentences. Also, you would need to understand session states and a few other things to run that..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CAA315M-b43"
      },
      "source": [
        "## On vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQosMfrsdbOR"
      },
      "source": [
        "Wait, hold on, where do such vectors come from? Well, they are pretrained form large text-collections in a \"self-supervised\" fashion. The idea comes from [Word2Vec](https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html) and related approaches, popular starting 2013. The idea here is to take text i.e. sentences and challenge a model to learn context, e.g. predict the \"masked\" next word from a sequence of words â‡’ Copenhagen is the [MASK] of Denmark."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOT7l8MFKdfH0tc/gPKr3na",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
