{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Messy Data to Actionable Insights: Predicting Hotel Booking Cancellations\n",
        "## A Real-World Binary Classification Pipeline with Business Impact Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running on a fresh environment, uncomment:\n",
        "# !pip install -q scikit-learn xgboost shap optuna seaborn joblib\n",
        "\n",
        "# Core data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn for preprocessing, modeling, and evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import calibration_curve, CalibrationDisplay\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    roc_auc_score, \n",
        "    roc_curve, \n",
        "    precision_recall_curve,\n",
        "    brier_score_loss,\n",
        "    ConfusionMatrixDisplay,\n",
        "    PrecisionRecallDisplay\n",
        ")\n",
        "\n",
        "# Advanced libraries\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import optuna\n",
        "\n",
        "# Settings for reproducibility and display\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set a professional plot style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## CORE (90 min): From data to calibrated, thresholded classifier\n",
        "\n",
        "### 1. The Business Problem: The Cost of Cancellations\n",
        "\n",
        "In the hotel industry, managing bookings is a high-stakes balancing act. A vacant room is lost revenue, but overbooking can lead to angry customers and reputational damage. Booking cancellations are a major source of this uncertainty. A reliable model that predicts which bookings are likely to be canceled can be a game-changer for a hotel's revenue management strategy.\n",
        "\n",
        "**Business Impact:**\n",
        "*   **Optimized Overbooking:** If a hotel can confidently predict cancellations, it can fine-tune its overbooking levels to maximize occupancy without inconveniencing guests.\n",
        "*   **Targeted Marketing:** High-risk bookings could be targeted with gentle reminders or non-refundable upgrade offers to \"lock in\" the reservation.\n",
        "*   **Improved Staffing & Resource Planning:** Accurate demand forecasting leads to better allocation of staff, cleaning services, and inventory.\n",
        "\n",
        "Our goal is to build a binary classification model to predict the value of the `is_canceled` column. This involves not just achieving high accuracy, but understanding the costs of different types of errors:\n",
        "-   **False Positive (Type I Error):** Predicting a cancellation that doesn't happen. The cost is potentially offering an unnecessary discount or irritating a committed customer.\n",
        "-   **False Negative (Type II Error):** Failing to predict a cancellation that does happen. The cost is lost revenue from an unexpectedly empty room.\n",
        "\n",
        "Often, the cost of a false negative is significantly higher than a false positive, a crucial consideration for our model.\n",
        "\n",
        "#### 1.1. Data Acquisition\n",
        "\n",
        "We will use a publicly available dataset on hotel bookings (Antonio, N., de Almeida, A., & Nunes, L. (2019). Hotel booking demand datasets. Data in brief, 22, 41-49. https://www.sciencedirect.com/science/article/pii/S2352340918315191). The data contains booking information for a city hotel and a resort hotel, including details like booking date, length of stay, number of guests, and whether the booking was ultimately canceled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data from a public repository to ensure reproducibility\n",
        "!wget -q https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv -O hotel_bookings.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df_raw = pd.read_csv('hotel_bookings.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a first impression of the data\n",
        "print(f\"Dataset shape: {df_raw.shape}\")\n",
        "df_raw.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.2. Initial Data Quality Assessment\n",
        "\n",
        "Let's quickly check for missing values and incorrect data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This initial check reveals:\n",
        "- **Missing Data:** The `company`, `agent`, and `country` columns have missing values. `company` is missing a very large percentage.\n",
        "- **Data Types:** Date information is split across multiple columns (`arrival_date_year`, `arrival_date_month`, etc.), which is not ideal for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the distribution of our target variable\n",
        "cancellation_counts = df_raw['is_canceled'].value_counts(normalize=True) * 100\n",
        "print(\"Cancellation Distribution:\")\n",
        "print(cancellation_counts)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='is_canceled', data=df_raw, palette='viridis')\n",
        "plt.title('Distribution of Cancellations (0 = Not Canceled, 1 = Canceled)')\n",
        "plt.ylabel('Number of Bookings')\n",
        "plt.xticks([0, 1], [f'Not Canceled\\n({cancellation_counts[0]:.1f}%)', f'Canceled\\n({cancellation_counts[1]:.1f}%)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a class imbalance, with about 37% of bookings being canceled. This isn't extreme, but it's significant enough that we must account for it in our modeling process. Standard accuracy can be misleading, and the model might become biased towards the majority class (not canceled).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Data Cleaning & Feature Engineering\n",
        "\n",
        "We'll now create a clean DataFrame `df` and engineer more powerful features to improve our model's performance.\n",
        "\n",
        "**ðŸŽ¯ Section Objectives:**\n",
        "\n",
        "-   Handle missing values with appropriate strategies.\n",
        "-   Create a unified `arrival_date` column.\n",
        "-   Engineer new features that capture booking behavior, seasonality, and guest characteristics.\n",
        "\n",
        "#### 2.1. Handling Missing Values\n",
        "\n",
        "We'll apply simple but effective strategies for the missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy to work with\n",
        "df = df_raw.copy()\n",
        "\n",
        "# For 'agent' and 'company', NaN might mean the booking was made directly.\n",
        "# We'll fill NaN with 0 for these ID columns.\n",
        "df['agent'].fillna(0, inplace=True)\n",
        "df['company'].fillna(0, inplace=True)\n",
        "\n",
        "# For 'country', a small number are missing. We'll fill with the mode ('PRT').\n",
        "df['country'].fillna(df['country'].mode()[0], inplace=True)\n",
        "\n",
        "# For 'children', NaN is rare. We'll assume 0 children.\n",
        "df['children'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2. Feature Engineering\n",
        "\n",
        "Creating new features from existing ones is often the key to building a high-performing model.\n",
        "\n",
        "##### Temporal Features\n",
        "Let's combine the date columns into a single `datetime` object and extract more granular time-based features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map month names to numbers\n",
        "month_map = {\n",
        "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
        "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
        "}\n",
        "df['arrival_date_month_num'] = df['arrival_date_month'].map(month_map)\n",
        "\n",
        "# Create the full arrival_date\n",
        "df['arrival_date'] = pd.to_datetime(\n",
        "    df['arrival_date_year'].astype(str) + '-' +\n",
        "    df['arrival_date_month_num'].astype(str) + '-' +\n",
        "    df['arrival_date_day_of_month'].astype(str),\n",
        "    errors='coerce' # Handle any invalid dates gracefully\n",
        ")\n",
        "\n",
        "# New seasonal/temporal features\n",
        "df['arrival_day_of_week'] = df['arrival_date'].dt.dayofweek # Monday=0, Sunday=6\n",
        "df['is_summer'] = df['arrival_date_month_num'].isin([6, 7, 8]).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Behavioral & Interaction Features\n",
        "We can combine columns to create more meaningful variables that capture guest behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Total number of guests\n",
        "df['total_guests'] = df['adults'] + df['children'] + df['babies']\n",
        "\n",
        "# Total stay duration in nights\n",
        "df['total_nights'] = df['stays_in_weekend_nights'] + df['stays_in_week_nights']\n",
        "\n",
        "# Create a binary feature for whether any deposit was made\n",
        "df['has_deposit'] = (df['deposit_type'] != 'No Deposit').astype(int)\n",
        "\n",
        "# Ratio of previous cancellations\n",
        "# Add a small epsilon to avoid division by zero\n",
        "df['previous_cancellation_rate'] = df['previous_cancellations'] / (df['previous_cancellations'] + df['previous_bookings_not_canceled'] + 1e-6)\n",
        "\n",
        "# It's unlikely a booking has 0 guests or 0 nights. Let's treat these as invalid and remove them.\n",
        "initial_rows = df.shape[0]\n",
        "df = df[(df['total_guests'] > 0) & (df['total_nights'] > 0)].copy()\n",
        "print(f\"Removed {initial_rows - df.shape[0]} rows with 0 guests or 0 nights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "âœ… **Checkpoint:** We've cleaned the data and engineered several new features related to seasonality, guest history, and booking duration. This enriched dataset should provide more signal for our model.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's explore the relationships between our features and the target variable, `is_canceled`.\n",
        "\n",
        "#### 3.1. What Drives Cancellations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(22, 14))\n",
        "\n",
        "# Lead Time vs. Cancellation (binned for readability)\n",
        "df_lt = df[['lead_time','is_canceled']].copy()\n",
        "df_lt['lead_time_bin'] = pd.qcut(df_lt['lead_time'], q=20, duplicates='drop')\n",
        "plot_df = df_lt.groupby('lead_time_bin', observed=True)['is_canceled'].mean().reset_index()\n",
        "sns.lineplot(data=plot_df, x=plot_df.index, y='is_canceled', ax=axes[0, 0], marker='o')\n",
        "axes[0, 0].set_title('Cancellation Rate vs. Lead Time (binned)')\n",
        "axes[0, 0].set_ylabel('Cancellation Probability')\n",
        "axes[0, 0].set_xlabel('Lead Time (Quantile Bins)')\n",
        "axes[0, 0].set_xticks([])\n",
        "\n",
        "\n",
        "# Deposit Type vs. Cancellation\n",
        "sns.barplot(x='deposit_type', y='is_canceled', data=df, palette='viridis', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Cancellation Rate by Deposit Type')\n",
        "axes[0, 1].set_ylabel('Cancellation Rate')\n",
        "axes[0, 1].set_xlabel('Deposit Type')\n",
        "\n",
        "# Repeated Guest vs. Cancellation\n",
        "sns.barplot(x='is_repeated_guest', y='is_canceled', data=df, palette='viridis', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Cancellation Rate for Repeated Guests')\n",
        "axes[1, 0].set_ylabel('Cancellation Rate')\n",
        "axes[1, 0].set_xlabel('Is Repeated Guest (0=No, 1=Yes)')\n",
        "\n",
        "# Cancellation Rate by Arrival Month\n",
        "monthly_cancellation = df.groupby('arrival_date_month_num')['is_canceled'].mean().reset_index()\n",
        "sns.lineplot(x='arrival_date_month_num', y='is_canceled', data=monthly_cancellation, marker='o', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Cancellation Rate by Arrival Month')\n",
        "axes[1, 1].set_ylabel('Cancellation Rate')\n",
        "axes[1, 1].set_xlabel('Month of Arrival')\n",
        "axes[1, 1].set_xticks(range(1, 13))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2. Correlation Analysis\n",
        "\n",
        "A correlation heatmap helps us understand linear relationships between numerical features and identify potential multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a subset of numeric features for the heatmap for clarity\n",
        "numeric_subset = [\n",
        "    'lead_time', 'total_guests', 'total_nights', 'previous_cancellations',\n",
        "    'booking_changes', 'adr', 'total_of_special_requests', 'is_canceled'\n",
        "]\n",
        "corr_matrix = df[numeric_subset].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of Key Numeric Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Insights from EDA:**\n",
        "1.  **Lead Time is Critical:** The longer a booking is made in advance, the higher the probability of cancellation. This is the strongest single indicator from our initial plots.\n",
        "2.  **Deposits are a Powerful Deterrent:** \"No Deposit\" bookings have a much higher cancellation rate. The \"Non Refund\" category is an interesting edge case; its near-100% cancellation rate might reflect how no-shows on non-refundable bookings are recorded.\n",
        "3.  **Loyalty Pays Off:** Repeated guests are far less likely to cancel. They are more familiar with the hotel and likely have more concrete travel plans.\n",
        "4.  **Seasonality Matters:** Cancellation rates fluctuate throughout the year, peaking in early summer and dropping in the autumn. This could inform seasonal marketing campaigns.\n",
        "5.  **Behavioral Signals:** The correlation matrix shows that `total_of_special_requests` has a negative correlation with `is_canceled`. This confirms our intuition: guests who make requests are more invested in their stay.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Temporal Split\n",
        "\n",
        "For this kind of booking data, a random split is dangerous. It can lead to **data leakage**, where the model learns from future information to predict the past. A **temporal split** is much more robust. We will sort the data by arrival date and use earlier bookings for training and later bookings for testing.\n",
        "\n",
        "> **âš ï¸ Common Pitfall: Data Leakage in Time-Series Data**\n",
        "> Using a standard `train_test_split` on time-ordered data like bookings is a frequent and serious mistake. A random split would mean your training set could contain bookings from August 2017, while your test set has bookings from January 2017. The model would learn patterns from the future to predict the pastâ€”something impossible in a real-world deployment. A temporal split ensures our model evaluation realistically simulates predicting future bookings based only on past data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final feature selection for our model\n",
        "# Note: 'agent' is an ID and can behave like a noisy proxy; for purity, consider dropping it.\n",
        "# We'll leave it in for now and revisit in advanced sessions.\n",
        "numeric_features = [\n",
        "    'lead_time', 'arrival_date_week_number', 'arrival_date_day_of_month',\n",
        "    'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children',\n",
        "    'babies', 'is_repeated_guest', 'previous_cancellations',\n",
        "    'previous_bookings_not_canceled', 'booking_changes', 'agent',\n",
        "    'days_in_waiting_list', 'adr', 'required_car_parking_spaces',\n",
        "    'total_of_special_requests', 'total_guests', 'total_nights',\n",
        "    'is_summer', 'previous_cancellation_rate'\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    'hotel', 'meal', 'market_segment', 'distribution_channel',\n",
        "    'reserved_room_type', 'deposit_type', 'customer_type'\n",
        "]\n",
        "\n",
        "# Our target variable\n",
        "target = 'is_canceled'\n",
        "\n",
        "# Drop rows where arrival_date is missing (from our datetime conversion)\n",
        "df_model = df.dropna(subset=['arrival_date']).copy()\n",
        "\n",
        "# Sort by date to prepare for temporal split\n",
        "df_model = df_model.sort_values('arrival_date')\n",
        "\n",
        "X = df_model[numeric_features + categorical_features]\n",
        "y = df_model[target]\n",
        "\n",
        "# Perform the temporal split\n",
        "# We'll use the first 70% of data for training, the next 15% for validation, and the last 15% for testing.\n",
        "train_size = int(len(X) * 0.70)\n",
        "val_size = int(len(X) * 0.15)\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the shapes and time ranges\n",
        "print(f\"Training set shape:   {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")\n",
        "print(f\"Test set shape:       {X_test.shape}\\n\")\n",
        "print(f\"Training data goes up to: {df_model.iloc[train_size - 1]['arrival_date'].date()}\")\n",
        "print(f\"Validation data from:     {df_model.iloc[train_size]['arrival_date'].date()} to {df_model.iloc[train_size + val_size - 1]['arrival_date'].date()}\")\n",
        "print(f\"Test data from:           {df_model.iloc[train_size + val_size]['arrival_date'].date()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5. Preprocessing Pipeline\n",
        "\n",
        "We'll use a `scikit-learn` pipeline to ensure our preprocessing steps (imputation, scaling, encoding) are applied consistently and without data leakage. This is crucial for creating a robust and deployable model.\n",
        "\n",
        "> **ðŸ’¡ Pro Tip: Why a Pipeline?**\n",
        "> A pipeline bundles preprocessing and modeling steps into a single object. This has several advantages:\n",
        "> 1.  **Prevents Data Leakage:** It ensures that information from the validation or test sets (like their mean or median) is never used to transform the training set.\n",
        "> 2.  **Simplifies Workflow:** You only need to call `.fit()` and `.predict()` on the pipeline object, which handles all the intermediate steps automatically.\n",
        "> 3.  **Ensures Consistency:** The exact same transformations are applied during training, evaluation, and future deployment, preventing subtle bugs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the numeric pipeline\n",
        "# We use 'median' for imputation as it's robust to outliers often found in fields like 'adr'.\n",
        "# StandardScaler standardizes features to have mean 0 and variance 1, which is important for linear models.\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Create the categorical pipeline\n",
        "# 'most_frequent' imputation is a safe choice for categorical data.\n",
        "# OneHotEncoder converts categories into a numerical format the model can understand, without implying an order.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Assemble the preprocessor using ColumnTransformer\n",
        "# This object applies the correct pipeline to the correct columns.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Models: Dummy â†’ LR â†’ RF â†’ XGB\n",
        "\n",
        "We'll start with baselines and train several candidate models. Our primary evaluation metric will be **ROC AUC**, which is a good choice for imbalanced classification problems. For tree-based models, we'll use `scale_pos_weight` to handle the class imbalance by giving more weight to the minority class (cancellations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Baseline Model (Dummy Classifier) ---\n",
        "dummy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('classifier', DummyClassifier(strategy='stratified'))])\n",
        "dummy_pipeline.fit(X_train, y_train)\n",
        "y_pred_dummy_proba = dummy_pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_dummy = roc_auc_score(y_val, y_pred_dummy_proba)\n",
        "print(f\"Baseline (Dummy) ROC AUC: {roc_auc_dummy:.4f}\")\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "logreg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                  ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))])\n",
        "logreg_pipeline.fit(X_train, y_train)\n",
        "y_pred_logreg_proba = logreg_pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_logreg = roc_auc_score(y_val, y_pred_logreg_proba)\n",
        "print(f\"Logistic Regression ROC AUC: {roc_auc_logreg:.4f}\")\n",
        "\n",
        "# --- Random Forest ---\n",
        "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))])\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred_rf_proba = rf_pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_rf = roc_auc_score(y_val, y_pred_rf_proba)\n",
        "print(f\"Random Forest ROC AUC: {roc_auc_rf:.4f}\")\n",
        "\n",
        "\n",
        "# --- XGBoost (Initial) ---\n",
        "# Calculate scale_pos_weight for handling imbalance\n",
        "neg, pos = np.bincount(y_train)\n",
        "scale_pos_weight = neg / pos\n",
        "print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
        "\n",
        "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', xgb.XGBClassifier(objective='binary:logistic', random_state=42, \n",
        "                                                                use_label_encoder=False, eval_metric='logloss',\n",
        "                                                                scale_pos_weight=scale_pos_weight))])\n",
        "# We will train our final model later after choosing it\n",
        "# For now, let's just create the pipeline and train it on the train set for evaluation\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_pred_xgb_proba = xgb_pipeline.predict_proba(X_val)[:, 1]\n",
        "roc_auc_xgb = roc_auc_score(y_val, y_pred_xgb_proba)\n",
        "print(f\"XGBoost (Initial) ROC AUC: {roc_auc_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Selection:** Both Random Forest and XGBoost performed very well. We will proceed with **XGBoost**. It offers more parameters for fine-tuning and often has a slight edge in performance. For the remainder of the CORE section, we will use this initial XGBoost model. In the PLUS section, we will tune it further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create final pipeline with default XGBoost for now\n",
        "# Train on combined train + validation data for final evaluation on test set\n",
        "X_train_full = pd.concat([X_train, X_val])\n",
        "y_train_full = pd.concat([y_train, y_val])\n",
        "\n",
        "final_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', xgb.XGBClassifier(objective='binary:logistic', random_state=42, \n",
        "                                                                use_label_encoder=False, eval_metric='logloss',\n",
        "                                                                scale_pos_weight=scale_pos_weight))])\n",
        "\n",
        "final_model_pipeline.fit(X_train_full, y_train_full)\n",
        "y_pred_final_proba = final_model_pipeline.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.5. The ROC Curve and AUC: Visualizing Predictive Power\n",
        "\n",
        "Our model produces a *probability* of cancellation, a risk score from 0% to 100%. The **ROC (Receiver Operating Characteristic) Curve** visualizes the trade-off between the True Positive Rate (catching actual cancellations) and the False Positive Rate (incorrectly flagging safe bookings) as we vary the decision threshold.\n",
        "\n",
        "The Area Under the Curve (AUC) summarizes this performance. A score of 0.5 is no better than a random guess, while 1.0 is perfect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plotting the ROC Curve ---\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_final_proba)\n",
        "final_roc_auc = roc_auc_score(y_test, y_pred_final_proba)\n",
        "\n",
        "print(f\"Final Model ROC AUC on Test Set: {final_roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {final_roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=14)\n",
        "plt.ylabel('True Positive Rate (Recall/Sensitivity)', fontsize=14)\n",
        "plt.title('ROC Curve for Cancellation Prediction', fontsize=16)\n",
        "plt.legend(loc=\"lower right\", fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.6. Probability Calibration: Can We Trust the Scores?\n",
        "\n",
        "A high AUC is great, but for business decisions, we also need the model's probabilities to be **well-calibrated**. This means that if the model predicts an 80% cancellation risk, roughly 80% of those bookings should actually cancel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "brier = brier_score_loss(y_test, y_pred_final_proba)\n",
        "print(f\"Brier Score Loss: {brier:.4f} (lower is better)\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "CalibrationDisplay.from_predictions(y_test, y_pred_final_proba, n_bins=10, ax=ax, strategy='uniform')\n",
        "ax.set_title(\"Calibration Plot (Reliability Diagram)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:** Our model's calibration curve is quite close to the ideal diagonal line, and the Brier score is low. This gives us confidence that the predicted probabilities are reliable and can be used directly for tasks like calculating expected revenue loss.\n",
        "\n",
        "#### 6.7. The Precision-Recall Trade-off\n",
        "\n",
        "While the ROC curve is excellent for overall model evaluation, the **Precision-Recall (P-R) Curve** is often more informative for imbalanced datasets and for business problems where the positive class (cancellations) is the main focus.\n",
        "\n",
        "*   **Precision:** *\"When the model predicts a cancellation, how often is it right?\"*\n",
        "*   **Recall:** *\"Of all the bookings that were actually canceled, what percentage did our model catch?\"*\n",
        "\n",
        "The P-R curve helps us visualize the trade-off as we adjust the probability threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred_final_proba, ax=ax)\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Application:** Choosing the right threshold is a business decision.\n",
        "*   **High-Precision Strategy:** Use a high threshold if the cost of acting on a false positive is high (e.g., offering a large, unnecessary discount).\n",
        "*   **High-Recall Strategy:** Use a low threshold if the cost of a missed cancellation (a false negative) is high (e.g., an unexpectedly empty room).\n",
        "\n",
        "#### 6.8. Cost-Based Thresholding\n",
        "\n",
        "We can formalize the threshold selection by defining the costs of our errors and finding the threshold that minimizes the total expected cost on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Cost-Based Thresholding ---\n",
        "# Define business costs (edit these in class)\n",
        "C_FP = 50   # cost of acting on a non-canceler (e.g., offering an unnecessary discount)\n",
        "C_FN = 200  # cost of missing a true cancelation (e.g., lost revenue from an empty room)\n",
        "\n",
        "thresholds = np.linspace(0, 1, 201)\n",
        "\n",
        "def expected_cost(y_true, p, t, c_fp=C_FP, c_fn=C_FN):\n",
        "    y_hat = (p >= t).astype(int)\n",
        "    FP = ((y_true == 0) & (y_hat == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_hat == 0)).sum()\n",
        "    return FP * c_fp + FN * c_fn\n",
        "\n",
        "costs = np.array([expected_cost(y_test.values, y_pred_final_proba, t) for t in thresholds])\n",
        "t_star = float(thresholds[costs.argmin()])\n",
        "\n",
        "print(f\"Cost-minimizing threshold: {t_star:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.plot(thresholds, costs)\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Expected Cost\")\n",
        "plt.title(\"Expected Cost vs Threshold\")\n",
        "plt.grid(True)\n",
        "plt.axvline(x=t_star, color='r', linestyle='--', label=f'Optimal Threshold = {t_star:.2f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we evaluate our model's performance using this business-aware threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate at t_star\n",
        "y_pred_star = (y_pred_final_proba >= t_star).astype(int)\n",
        "print(f\"Classification Report on Test Set (Threshold = {t_star:.2f}):\")\n",
        "print(classification_report(y_test, y_pred_star))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_star, normalize='true', ax=ax)\n",
        "ax.set_title(f'Confusion Matrix (Threshold = {t_star:.2f})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.9. From Probabilities to Forecasts\n",
        "\n",
        "Because our model's probabilities are well-calibrated, we can aggregate them to create powerful forecasts. For example, we can estimate the number of cancellations expected on any given night. This is a direct input for dynamic overbooking strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected cancellations per night on the test period (using calibrated probabilities)\n",
        "nightly_expected_cancels = (\n",
        "    df_model.loc[y_test.index, ['arrival_date']]\n",
        "    .assign(p=y_pred_final_proba)\n",
        "    .groupby('arrival_date')['p']\n",
        "    .sum()\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "print(\"Sample of expected nightly cancellations:\")\n",
        "nightly_expected_cancels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PLUS (Optional): Tuning, Explainability, Deployment\n",
        "\n",
        "### 7. Optuna Tuning\n",
        "\n",
        "Run if you want to push AUC further; default trials are small. We'll use `Optuna` to automatically find the best combination of hyperparameters for our XGBoost model, optimizing for the ROC AUC score on our validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the objective function for Optuna\n",
        "# This function takes a 'trial' object, which suggests hyperparameter values.\n",
        "# It then trains a model with these values and returns a performance score.\n",
        "def objective(trial):\n",
        "    # We must fit the preprocessor on training data first to avoid data leakage\n",
        "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "    X_val_transformed = preprocessor.transform(X_val)\n",
        "    \n",
        "    # Define the search space for hyperparameters\n",
        "    # Optuna intelligently samples from these ranges to find the best combination.\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'objective': 'binary:logistic',\n",
        "        'random_state': 42,\n",
        "        'use_label_encoder': False,\n",
        "        'eval_metric': 'logloss',\n",
        "        'scale_pos_weight': scale_pos_weight\n",
        "    }\n",
        "    \n",
        "    # Train the model with the suggested parameters\n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(X_train_transformed, y_train)\n",
        "    \n",
        "    # Evaluate on the validation set\n",
        "    y_pred_proba = model.predict_proba(X_val_transformed)[:, 1]\n",
        "    auc = roc_auc_score(y_val, y_pred_proba)\n",
        "    \n",
        "    return auc\n",
        "\n",
        "# Create a study and optimize (run a small number of trials for the workshop)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# Note: For a real project, n_trials would be much larger (e.g., 50-100).\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(f\"Best trial ROC AUC: {study.best_value:.4f}\")\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "# Visualize the optimization history\n",
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After tuning, we would create a new `final_model_pipeline` with these `best_params` and retrain it on the full training data (`X_train_full`, `y_train_full`) before proceeding to the next steps. For simplicity, we will continue using the untuned model for the explainability section.\n",
        "\n",
        "### 8. SHAP\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) goes deeper than standard feature importance, showing us the impact of each feature for every single prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Get the fitted preprocessor and classifier from our (untuned) final pipeline\n",
        "fitted_preprocessor = final_model_pipeline.named_steps['preprocessor']\n",
        "classifier = final_model_pipeline.named_steps['classifier']\n",
        "\n",
        "# 2. Transform the test data to get the final feature set\n",
        "X_test_transformed = pd.DataFrame(\n",
        "    fitted_preprocessor.transform(X_test),\n",
        "    columns=fitted_preprocessor.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# 3. (Optional) downsample for speed on modest hardware\n",
        "shap_sample_n = min(5000, X_test_transformed.shape[0])\n",
        "X_shap = X_test_transformed.iloc[:shap_sample_n]\n",
        "\n",
        "# 4. Create a SHAP explainer and get SHAP values\n",
        "explainer = shap.TreeExplainer(classifier)\n",
        "shap_values = explainer.shap_values(X_shap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 8.1. Global Feature Importance with Direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_shap, max_display=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoding the Drivers of Cancellation: From Data to Strategy\n",
        "\n",
        "This SHAP plot is the \"brain scan\" of our model. It reveals not just *what* features are important, but *how* they influence the prediction for every single booking.\n",
        "\n",
        "> **How to Read This Plot:**\n",
        ">\n",
        "> *   **Vertical Axis:** Features are ranked by their overall importance, top to bottom.\n",
        "> *   **Horizontal Axis (SHAP Value):** A positive value pushes the prediction towards **\"Canceled\"**, while a negative value pushes it towards **\"Not Canceled\"**.\n",
        "> *   **Color:** Red dots represent a high value for a feature (e.g., long lead time), while blue dots represent a low value (e.g., short lead time).\n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸš© Key Drivers of Cancellation (Risk Factors)\n",
        "\n",
        "1.  **Deposit Type: `Non Refund`**: A \"Non Refund\" deposit type (a red dot) strongly pushes the prediction to \"Canceled.\" This is likely a data artifact where no-shows on non-refundable rates are coded as canceled, but it confirms the immense power of financial commitment.\n",
        "2.  **Lead Time**: The longer the time between booking and arrival (red dots), the higher the cancellation risk.\n",
        "3.  **Market Segment: `Online TA` (Online Travel Agent)**: Bookings from OTAs (red dots) have a higher likelihood of cancellation, likely due to lenient OTA policies.\n",
        "\n",
        "---\n",
        "\n",
        "#### âœ… Key Indicators of a Committed Booking (Safety Factors)\n",
        "\n",
        "1.  **Total Special Requests**: A high number of special requests (red dots) results in a strong *negative* SHAP value, significantly lowering cancellation probability. This is a powerful behavioral signal of commitment.\n",
        "2.  **Required Car Parking Spaces**: A request for parking (red dot) has a large negative SHAP value, signaling a guest with confirmed travel logistics.\n",
        "3.  **Booking Changes**: Counterintuitively, a guest making changes to their booking (red dots) is a sign of engagement and refinement of plans, not abandonment.\n",
        "\n",
        "#### 8.2. Local Interpretability: Explaining a Single Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the booking with the highest predicted risk within our SHAP sample\n",
        "y_pred_shap_proba = y_pred_final_proba[:shap_sample_n]\n",
        "high_risk_idx_shap = np.argmax(y_pred_shap_proba)\n",
        "\n",
        "print(f\"Explaining booking with a {y_pred_shap_proba[high_risk_idx_shap]:.2%} predicted probability of cancellation.\")\n",
        "print(f\"Actual outcome: {'Canceled' if y_test.iloc[high_risk_idx_shap] == 1 else 'Not Canceled'}\")\n",
        "\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, \n",
        "                shap_values[high_risk_idx_shap, :], \n",
        "                X_shap.iloc[high_risk_idx_shap, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This force plot shows the \"push and pull\" of each feature for this specific booking. Features in red (like a long `lead_time`) are pushing the prediction higher (towards cancellation), while blue features would push it lower.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Deployment & Monitoring\n",
        "\n",
        "A model is only useful if it can be deployed into a production environment to make real-time predictions.\n",
        "\n",
        "#### 9.1. Saving the Model\n",
        "\n",
        "The first step is to save our entire trained pipeline (preprocessor + model) into a single file using `joblib`. This ensures that the exact same preprocessing steps are applied to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a version/date to the filename for better tracking\n",
        "model_filename = f'hotel_cancellation_model_{datetime.now().strftime(\"%Y%m%d\")}.pkl'\n",
        "\n",
        "# Save the final pipeline object\n",
        "joblib.dump(final_model_pipeline, model_filename)\n",
        "print(f\"Model pipeline saved successfully to '{model_filename}'\")\n",
        "\n",
        "# Example of loading the model back\n",
        "loaded_pipeline = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 9.2. Deployment Strategy\n",
        "\n",
        "Once saved, the model can be deployed in several ways:\n",
        "*   **API Endpoint:** The most common approach is to wrap the model in a web service (e.g., using Flask or FastAPI). The booking system can then send a `POST` request with the new booking's data to the API and receive the cancellation probability in real-time.\n",
        "*   **Batch Processing:** For less time-sensitive tasks like daily revenue forecasting, a script could run once a day, load the model, score all new bookings, and update a database with their risk scores.\n",
        "\n",
        "#### 9.3. Monitoring and Retraining\n",
        "\n",
        "Models are not static. The real world changes, and a model's performance can degrade over timeâ€”a phenomenon known as **model drift**.\n",
        "*   **Monitoring:** It's critical to log the model's predictions and, when the actual outcome is known (guest checks out or cancels), compare them. Key metrics like ROC AUC, precision, recall, and calibration should be tracked on a dashboard.\n",
        "*   **Retraining:** A retraining schedule should be established. This could be time-based (e.g., retrain every quarter on the latest data) or triggered by a significant drop in performance. The temporal split strategy used here should be replicated in the retraining pipeline to ensure valid evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Group Exercise\n",
        "\n",
        "**Tasks for Breakout Groups:**\n",
        "\n",
        "1.  **Advanced Feature Engineering:** The `reservation_status_date` is the date the final status (Canceled or Check-Out) was set. For canceled bookings, the time difference between the booking date and this `reservation_status_date` could be a powerful feature. Create a new feature called `cancellation_timing` (days between booking and cancellation) and discuss how you would incorporate it into the model *without causing data leakage*. How would this feature be used in a real-time prediction scenario?\n",
        "\n",
        "2.  **The Precision/Recall Business Trade-off:**\n",
        "    *   Our cost-based analysis gave us an optimal threshold. Let's explore others. Recalculate the confusion matrix and classification report using a **higher threshold of 0.7** (i.e., `y_pred = (y_pred_final_proba > 0.7).astype(int)`).\n",
        "    *   How did Precision and Recall for the 'Canceled' class change?\n",
        "    *   Describe a business scenario where this new, higher-precision model would be preferable, even though it catches fewer cancellations overall.\n",
        "    *   Now, do the same for a **lower threshold of 0.3**. When would this higher-recall model be the better business choice?\n",
        "\n",
        "3.  **Business Strategy Simulation:** A new booking has the following characteristics: `hotel='City Hotel'`, `lead_time=300`, `deposit_type='No Deposit'`, `customer_type='Transient'`, `total_of_special_requests=0`, and `market_segment='Online TA'`. Assume other key numeric features are at the training set median and other categoricals at the mode.\n",
        "    *   Create a pandas DataFrame for this single booking.\n",
        "    *   Use your final trained pipeline (`final_model_pipeline.predict_proba()`) to predict its cancellation probability.\n",
        "    *   Based on this risk score and the SHAP insights, design a 2-step intervention plan for this specific booking. What is the first action, and what is the follow-up? Be specific."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langcorn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
