{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPP5lrTrWwyhg6mQrn6Z+Ah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M4_timeseries_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yfinance"
      ],
      "metadata": {
        "id": "6rwW_JuPWfRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF2VPA-Axx2O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import seaborn as sns\n",
        "import altair as alt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# before starting, we define some constants\n",
        "figsize = (9, 6)\n",
        "lowest_q, low_q, high_q, highest_q = 0.01, 0.1, 0.9, 0.99\n",
        "label_q_outer = f\"{int(lowest_q * 100)}-{int(highest_q * 100)}th percentiles\"\n",
        "label_q_inner = f\"{int(low_q * 100)}-{int(high_q * 100)}th percentiles\""
      ],
      "metadata": {
        "id": "zzSLdmQpHGKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to deep learning (transformer-based) Timeseries Forecast"
      ],
      "metadata": {
        "id": "OSOqg2C-Gx5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **[NeuralProphet (Facebook)](https://arxiv.org/abs/2111.15397?fbclid=IwAR2vCkHYiy5yuPPjWXpJgAJs-uD5NkH4liORt1ch4a6X_kmpMqagGtXyez4)**\n",
        "  * Hybrid forecasting framework based on PyTorch\n",
        "  * Local context is introduced with auto-regression and covariate modules, which can be configured as classical linear regression or as Neural Networks\n",
        "  * Otherwise, NeuralProphet retains the design philosophy of Prophet and provides the same basic model components.\n",
        "* **[N-BEATS (ElementAI)](https://arxiv.org/abs/1905.10437):** Essentially, N-BEATS is a pure deep learning architecture based on a deep stack of ensembled feed forward networks that are also stacked by interconnecting backcast and forecast links.\n",
        "  * Easy to use: The model is simple to understand and has a modular structure (blocks and stacks). \n",
        "  * Multiple time-series: The model has the ability to generalize on many time-series.\n",
        "* **[N-HiTS (ElementAI)](https://arxiv.org/pdf/2201.12886.pdf):** Extension of N-BEATS model.\n",
        "  * Improves the accuracy of the predictions and reduces the computational cost. This is achieved by the model sampling the time series at different rates. \n",
        "  * Multi-rate signal sampling: the model can learn short-term and long-term effects in the series. \n",
        "* **[DeepAR (Amazon)](https://arxiv.org/abs/1704.04110?context=stat.ML):** A novel time series model that combines both deep-learning and autoregressive characteristics. \n",
        "  * Multiple time series: DeepAR works really well with multiple time series: A global model is built by using multiple time series with slightly different distributions.\n",
        "  * Rich set of inputs: Apart from historical data, DeepAR also allows the use of known future time sequences (a characteristic of auto-regressive models) and extra static attributes for series.\n",
        "  * Automatic scaling: In DeepAR, there is no need to do that manually since the model under the hood scales the autoregressive input.\n",
        "* **[Spacetimeformer:](https://arxiv.org/abs/2109.12218)** Considers both temporal and spatial relationships.\n",
        "  * Interesting when dealing with geospatial data, but I have little experience there.\n",
        "* **[Temporal Fusion Transformer](https://arxiv.org/abs/1912.09363):** Temporal Fusion Transformer (TFT) is a transformer-based time series forecasting model published by Google.\n",
        "  * Multiple time series: Like the aforementioned models, TFT supports building a model on multiple, heterogeneous time series.\n",
        "  * Rich number of features: TFT supports 3 types of features: i) time-dependent data with known inputs into the future ii) time-dependent data known only up to the present and iii) categorical/static variables, also known as time-invariant features. \n",
        "  * Interpretability: TFT gives much emphasis on interpretability. Specifically, by taking advantage of the Variable Selection component, the model can successfully measure the impact of each feature.\n",
        "  * Prediction Intervals: Similar to DeepAR, TFT outputs a prediction interval along with the predicted values, by using quantile regression."
      ],
      "metadata": {
        "id": "0pTvjTw8nYor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Fusion Transformers"
      ],
      "metadata": {
        "id": "oXFgP1uoNGug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Temporal Fusion Transformer (TFT) is an **attention-based Deep Neural Network**, optimized for great performance and interpretability. \n",
        "\n",
        "**Advantages and novelties:**\n",
        "\n",
        "* Rich features: \n",
        "  1. temporal data with known inputs into the future \n",
        "  2. temporal data known only up to the present and \n",
        "  3. exogenous categorical/static variables, also known as time-invariant features.\n",
        "* Heterogeneous time series: Supports training on multiple time series, splits processing into 2 parts: local processing which focuses on the characteristics of specific events and global processing which captures the collective characteristics of all time series.\n",
        "* Multi-horizon forecasting: Supports multi-step predictions. Apart from the actual prediction, TFT also outputs prediction intervals, by using the quantile loss function.\n",
        "* Interpretability: At its core, TFT is a transformer-based architecture. By taking advantage of self-attention, this model presents a novel Muti Head attention mechanism which when analyzed, provides extra insight on feature importances. "
      ],
      "metadata": {
        "id": "L8iSEax1ORax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format and structure\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/time_series_transformer"
      ],
      "metadata": {
        "id": "---80IBEnmqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFT Implementation high level (DARTS)\n",
        "\n",
        "[Darts](https://unit8co.github.io/darts/) is a Python library for easy manipulation and forecasting of time series. It contains a variety of models, from classics such as ARIMA to deep neural networks. The models can all be used in the same way, using fit() and predict() functions, similar to scikit-learn. The library also makes it easy to backtest models, combine the predictions of several models, and take external data into account. \n",
        "\n",
        "Darts supports both univariate and multivariate time series and models. The ML-based models can be trained on potentially large datasets containing multiple time series, and some of the models offer a rich support for probabilistic forecasting.\n",
        "\n",
        "While there is also a standalone version of TFT (eg. [standalone pytorch implementation](https://pypi.org/project/tft-torch/)), we will for this example use the Darts implementation, since it eases the integration of TFT in your traditional forecasting pipeline."
      ],
      "metadata": {
        "id": "9TF87dp1cNCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install darts"
      ],
      "metadata": {
        "id": "jSB7GXoV__ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from darts import TimeSeries, concatenate\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.models import TFTModel\n",
        "from darts.metrics import mape, rmse\n",
        "\n",
        "from darts.utils.statistics import check_seasonality, plot_acf\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "from darts.utils.likelihood_models import QuantileRegression\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "sOUbckxkq5WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Darts’ TFTModel incorporates the following main components from the original Temporal Fusion Transformer (TFT) architecture:\n",
        "\n",
        "* gating mechanisms: skip over unused components of the model architecture\n",
        "* variable selection networks: select relevant input variables at each time step.\n",
        "* temporal processing of past and future input with LSTMs (long short-term memory)\n",
        "* multi-head attention: captures long-term temporal dependencies\n",
        "* prediction intervals: per default, produces quantile forecasts instead of deterministic values"
      ],
      "metadata": {
        "id": "ri_OK8zJcLe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "TFTModel can be trained with past and future covariates. It is trained sequentially on fixed-size chunks consisting of an encoder and a decoder part:\n",
        "\n",
        "* encoder: past input with input_chunk_length\n",
        "  * past target: mandatory\n",
        "  * past covariates: optional\n",
        "* decoder: future known input with output_chunk_length\n",
        "  * future covariates: mandatory (if none are available, consider TFTModel’s optional arguments add_encoders or add_relative_index from here)\n",
        "\n",
        "In each iteration, the model produces a quantile prediction of shape (output_chunk_length, n_quantiles) on the decoder part."
      ],
      "metadata": {
        "id": "nVb3GBVjrHLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecast\n",
        "\n",
        "Per default, TFTModel produces probabilistic quantile forecasts using QuantileRegression. This gives the range of likely target values at each prediction step. Most deep learning models in Darts’ - including TFTModel - support QuantileRegression and 16 other likelihoods to produce probabilistic forecasts by setting likelihood=MyLikelihood() at model creation."
      ],
      "metadata": {
        "id": "IMpsYb3ErnjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy example (Air Passangers)\n",
        "\n",
        "Adopted from the [DARTS pakage tutorial](https://unit8co.github.io/darts/examples/13-TFT-examples.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "eDCfU0H7qeGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set that is highly dependent on covariates. Knowing the month tells us a lot about the seasonal component, whereas the year determines the effect of the trend component.\n",
        "\n",
        "Additionally, let’s convert the time index to integer values and use them as covariates as well.\n",
        "\n",
        "All of the three covariates are known in the future, and can be used as future_covariates with the TFTModel."
      ],
      "metadata": {
        "id": "nWfNzvjvrZo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "from darts.datasets import AirPassengersDataset\n",
        "\n",
        "series = AirPassengersDataset().load()"
      ],
      "metadata": {
        "id": "x59sstycMDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series.head()"
      ],
      "metadata": {
        "id": "yRodLz0uaqXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert monthly number of passengers to average daily number of passengers per month\n",
        "series = series / TimeSeries.from_series(series.time_index.days_in_month)\n",
        "series = series.astype(np.float32)"
      ],
      "metadata": {
        "id": "sjGTWStca2Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation sets:\n",
        "training_cutoff = pd.Timestamp(\"19571201\")\n",
        "train, val = series.split_after(training_cutoff)"
      ],
      "metadata": {
        "id": "2Jbc0q8Ma5jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
        "transformer = Scaler()\n",
        "train_transformed = transformer.fit_transform(train)\n",
        "val_transformed = transformer.transform(val)\n",
        "series_transformed = transformer.transform(series)"
      ],
      "metadata": {
        "id": "XKnI5IyMa-9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create year, month and integer index covariate series\n",
        "covariates = datetime_attribute_timeseries(series, attribute=\"year\", one_hot=False)"
      ],
      "metadata": {
        "id": "M7RbIfeybEKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariates = covariates.stack(datetime_attribute_timeseries(series, attribute=\"month\", one_hot=False))"
      ],
      "metadata": {
        "id": "CIcY6-eMbPiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariates = covariates.stack(\n",
        "    TimeSeries.from_times_and_values(\n",
        "        times=series.time_index,\n",
        "        values=np.arange(len(series)),\n",
        "        columns=[\"linear_increase\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "covariates = covariates.astype(np.float32)"
      ],
      "metadata": {
        "id": "06KqSxxobTt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covariates"
      ],
      "metadata": {
        "id": "K2WNDnBlD42_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov_train, cov_val = covariates.split_after(training_cutoff)"
      ],
      "metadata": {
        "id": "pgXzx74Qbise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform covariates (note: we fit the transformer on train split and can then transform the entire covariates series)\n",
        "scaler_covs = Scaler()\n",
        "scaler_covs.fit(cov_train)\n",
        "covariates_transformed = scaler_covs.transform(covariates)"
      ],
      "metadata": {
        "id": "zkrj3fje6liS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TFTModel can only be used if some future input is given. Optional parameters add_encoders and add_relative_index can be useful, especially if we don’t have any future input available. They generate endoded temporal data is used as future covariates.\n",
        "\n",
        "Since we already have future covariates defined in our example they are commented out."
      ],
      "metadata": {
        "id": "EeHOKKY7sWYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 200\n",
        "input_chunk_length = 24\n",
        "forecast_horizon = 12"
      ],
      "metadata": {
        "id": "AnMc0Tr37HA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = TFTModel(\n",
        "    input_chunk_length=input_chunk_length,\n",
        "    output_chunk_length=forecast_horizon,\n",
        "    hidden_size=64,\n",
        "    lstm_layers=1,\n",
        "    num_attention_heads=4,\n",
        "    dropout=0.1,\n",
        "    batch_size=16,\n",
        "    n_epochs=100,\n",
        "    add_relative_index=False,\n",
        "    add_encoders=None,\n",
        "    likelihood=QuantileRegression(\n",
        "        # quantiles= [ 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
        "    ),  # QuantileRegression is set per default\n",
        "    # loss_fn=MSELoss(),\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "ku5pnZ8L7Nal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what follows, we can just provide the whole covariates series as future_covariates argument to the model; the model will slice these covariates and use only what it needs in order to train on forecasting the target train_transformed:"
      ],
      "metadata": {
        "id": "7fHejuSQsj06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.fit(train_transformed, future_covariates=covariates_transformed, verbose=True)"
      ],
      "metadata": {
        "id": "7Oo9tYK37Sxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform a one-shot prediction of 24 months using the “current” model - i.e., the model at the end of the training procedure:"
      ],
      "metadata": {
        "id": "b2dBxj_Rsscc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, n, actual_series, val_series):\n",
        "    pred_series = model.predict(n=n, num_samples=num_samples)\n",
        "\n",
        "    # plot actual series\n",
        "    plt.figure(figsize=figsize)\n",
        "    actual_series[: pred_series.end_time()].plot(label=\"actual\")\n",
        "\n",
        "    # plot prediction with quantile ranges\n",
        "    pred_series.plot(\n",
        "        low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer\n",
        "    )\n",
        "    pred_series.plot(low_quantile=low_q, high_quantile=high_q, label=label_q_inner)\n",
        "\n",
        "    plt.title(\"MAPE: {:.2f}%\".format(mape(val_series, pred_series)))\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "HGpnsOFl7haz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(my_model, 24, series_transformed, val_transformed)"
      ],
      "metadata": {
        "id": "-OAUz7T37m9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s backtest our TFTModel model, to see how it performs with a forecast horizon of 12 months over the last 3 years:"
      ],
      "metadata": {
        "id": "VTOLk8SPsyAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backtest_series = my_model.historical_forecasts(\n",
        "    series_transformed,\n",
        "    future_covariates=covariates_transformed,\n",
        "    start=train.end_time() + train.freq,\n",
        "    num_samples=num_samples,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    stride=forecast_horizon,\n",
        "    last_points_only=False,\n",
        "    retrain=False,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "e3fskAmPs33Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_backtest(backtest_series, actual_series, horizon, start, transformer):\n",
        "    plt.figure(figsize=figsize)\n",
        "    actual_series.plot(label=\"actual\")\n",
        "    backtest_series.plot(\n",
        "        low_quantile=lowest_q, high_quantile=highest_q, label=label_q_outer\n",
        "    )\n",
        "    backtest_series.plot(low_quantile=low_q, high_quantile=high_q, label=label_q_inner)\n",
        "    plt.legend()\n",
        "    plt.title(f\"Backtest, starting {start}, {horizon}-months horizon\")\n",
        "    print(\n",
        "        \"MAPE: {:.2f}%\".format(\n",
        "            mape(\n",
        "                transformer.inverse_transform(actual_series),\n",
        "                transformer.inverse_transform(backtest_series),\n",
        "            )\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "oS8JNCS8s89h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_backtest(\n",
        "    backtest_series=concatenate(backtest_series),\n",
        "    actual_series=series_transformed,\n",
        "    horizon=forecast_horizon,\n",
        "    start=training_cutoff,\n",
        "    transformer=transformer,\n",
        ")"
      ],
      "metadata": {
        "id": "8WlzMutZtAlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch TFT implementation\n",
        "\n",
        "Example partially adapted from [pytorch-forecasting](https://pytorch-forecasting.readthedocs.io/)"
      ],
      "metadata": {
        "id": "vKNUHgtvFV2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install old version of pytorch, since current update causes problems (only temorary, soon probably not necessary anymore)\n",
        "!pip install pytorch_lightning==1.9.0"
      ],
      "metadata": {
        "id": "5EXkoyVFXoJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_forecasting"
      ],
      "metadata": {
        "id": "MWpZKikMX6wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "\n",
        "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
      ],
      "metadata": {
        "id": "1IOhF9nxFnxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "9vAsT_eYHvlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data structure basics"
      ],
      "metadata": {
        "id": "t_Ylh23sKdcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = pd.DataFrame(\n",
        "    dict(\n",
        "        time_idx=np.tile(np.arange(6), 3),\n",
        "        target=np.array([0,1,2,3,4,5,20,21,22,23,24,25,40,41,42,43,44,45]),\n",
        "        group=np.repeat(np.arange(3), 6),\n",
        "        holidays = np.tile(['X','Black Friday', 'X','Christmas','X', 'X'],3),\n",
        "    )\n",
        ")\n",
        "example_data"
      ],
      "metadata": {
        "id": "29fFgImQIvf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_encode = 2\n",
        "n_predict = 3 \n",
        "\n",
        "# create the time-series dataset from the pandas df\n",
        "dataset = TimeSeriesDataSet(\n",
        "    example_data,\n",
        "    group_ids=[\"group\"],\n",
        "    target=\"target\",\n",
        "    time_idx=\"time_idx\",\n",
        "    max_encoder_length= n_encode,\n",
        "    max_prediction_length=n_predict,\n",
        "    time_varying_unknown_reals=[\"target\"],\n",
        "    static_categoricals=[\"holidays\"],\n",
        "    target_normalizer=None\n",
        ")"
      ],
      "metadata": {
        "id": "Ddzv4eHXKhvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the dataset to a dataloader\n",
        "dataloader = dataset.to_dataloader(batch_size=1)\n",
        "\n",
        "#load the first batch\n",
        "x, y = next(iter(dataloader))\n",
        "\n",
        "x"
      ],
      "metadata": {
        "id": "mGNayaTEKy4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beer Sales data"
      ],
      "metadata": {
        "id": "pNObRXISMW9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the [Stallion dataset from Kaggle](https://www.kaggle.com/datasets/utathya/future-volume-prediction) describing sales of various beverages. Our task is to make a six-month forecast of the sold volume by stock keeping units (SKU), that is products, sold by an agency, that is a store. "
      ],
      "metadata": {
        "id": "prDNQfrbOQB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_forecasting.data.examples import get_stallion_data\n",
        "data = get_stallion_data()\n",
        "data.head()"
      ],
      "metadata": {
        "id": "DI14z2HqHiEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "The dataset is already in the correct format but misses some important features. Most importantly, we need to add a time index that is incremented by one for each time step. Further, it is beneficial to add date features, which in this case means extracting the month from the date record."
      ],
      "metadata": {
        "id": "CT9bx8FYOhUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add time index\n",
        "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
        "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
        "\n",
        "# add additional features\n",
        "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
        "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
        "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
        "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
        "\n",
        "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
        "special_days = [\n",
        "    \"easter_day\",\n",
        "    \"good_friday\",\n",
        "    \"new_year\",\n",
        "    \"christmas\",\n",
        "    \"labor_day\",\n",
        "    \"independence_day\",\n",
        "    \"revolution_day_memorial\",\n",
        "    \"regional_games\",\n",
        "    \"fifa_u_17_world_cup\",\n",
        "    \"football_gold_cup\",\n",
        "    \"beer_capital\",\n",
        "    \"music_fest\",\n",
        "]\n",
        "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")"
      ],
      "metadata": {
        "id": "BmWhgwIXMa5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(10, random_state=1337)"
      ],
      "metadata": {
        "id": "xpsQinPFN-_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().T"
      ],
      "metadata": {
        "id": "6NkXXcX7PiIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset and dataloaders"
      ],
      "metadata": {
        "id": "e_nVeQIpRlg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_prediction_length = 6\n",
        "max_encoder_length = 24\n",
        "training_cutoff = data[\"time_idx\"].max() - max_prediction_length"
      ],
      "metadata": {
        "id": "xQCIrqI3SLQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = TimeSeriesDataSet(\n",
        "    data[lambda x: x.time_idx <= training_cutoff],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"volume\",\n",
        "    group_ids=[\"agency\", \"sku\"],\n",
        "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"agency\", \"sku\"],\n",
        "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
        "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
        "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
        "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=[\n",
        "        \"volume\",\n",
        "        \"log_volume\",\n",
        "        \"industry_volume\",\n",
        "        \"soda_volume\",\n",
        "        \"avg_max_temp\",\n",
        "        \"avg_volume_by_agency\",\n",
        "        \"avg_volume_by_sku\",\n",
        "    ],\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
        "    ),  # use softplus and normalize by group\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QxHpk_QLR2d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)"
      ],
      "metadata": {
        "id": "CzJq83vyR2qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloaders for model\n",
        "batch_size = 128  # set this between 32 to 128\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
      ],
      "metadata": {
        "id": "IZ2wy4ThR25S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create baseline model\n",
        "\n",
        "Evaluating a Baseline model that predicts the next 6 months by simply repeating the last observed volume gives us a simle benchmark that we want to outperform."
      ],
      "metadata": {
        "id": "u5A0ummnSxmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
        "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
        "baseline_predictions = Baseline().predict(val_dataloader)\n",
        "(actuals - baseline_predictions).abs().mean().item()"
      ],
      "metadata": {
        "id": "KTqblX74Taep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Temporal Fusion Transformer"
      ],
      "metadata": {
        "id": "bakh0oXPV9Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure network and trainer\n",
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(\n",
        "    gpus=0,\n",
        "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
        "    # of the gradient for recurrent neural networks\n",
        "    gradient_clip_val=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "BbZ2zQbAV8AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding optimal learning rate"
      ],
      "metadata": {
        "id": "KFlDFFW4Z0B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    # not meaningful for finding the learning rate but otherwise very important\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
        "    # number of attention heads. Set to up to 4 for large datasets\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
        "    hidden_continuous_size=8,  # set to <= hidden_size\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    # reduce learning rate if no improvement in validation loss after x epochs\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ],
      "metadata": {
        "id": "pGnzrvQwWwE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find optimal learning rate\n",
        "# NOTE: Currently some error, i dont know why. I hope it is due to the recent TFT/ Pytorch lightning update and gets fixed soon \n",
        "res = trainer.tuner.lr_find(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        "    max_lr=10.0,\n",
        "    min_lr=1e-6,\n",
        ")"
      ],
      "metadata": {
        "id": "4tV2DUCQUEGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRain model"
      ],
      "metadata": {
        "id": "jS5SJyH_aQta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure network and trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "n_epochs = 30"
      ],
      "metadata": {
        "id": "cbe_w6jcauIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=n_epochs,\n",
        "    gpus=0,\n",
        "    enable_model_summary=True,\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")"
      ],
      "metadata": {
        "id": "j6g64a-eauPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=7,  # 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
        "    reduce_on_plateau_patience=4,\n",
        ")\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ],
      "metadata": {
        "id": "ptT2DZoaadVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit network\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")"
      ],
      "metadata": {
        "id": "8z1VWniZaitH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "Would amke sense, but takes too lomng at this case..."
      ],
      "metadata": {
        "id": "vVZMxdojg9Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "#from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "# # create study\n",
        "#study = optimize_hyperparameters(\n",
        "#    train_dataloader,\n",
        "#    val_dataloader,\n",
        "#    model_path=\"optuna_test\",\n",
        "#    n_trials=200,\n",
        "#    max_epochs=50,\n",
        "#    gradient_clip_val_range=(0.01, 1.0),\n",
        "#    hidden_size_range=(8, 128),\n",
        "#    hidden_continuous_size_range=(8, 128),\n",
        "#    attention_head_size_range=(1, 4),\n",
        "#    learning_rate_range=(0.001, 0.1),\n",
        "#    dropout_range=(0.1, 0.3),\n",
        "#    trainer_kwargs=dict(limit_train_batches=30),\n",
        "#    reduce_on_plateau_patience=4,\n",
        "#    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
        "#)\n",
        "\n",
        "# save study results - also we can resume tuning at a later point in time\n",
        "#with open(\"test_study.pkl\", \"wb\") as fout:\n",
        "#    pickle.dump(study, fout)\n",
        "\n",
        "## show best hyperparameters\n",
        "#print(study.best_trial.params)"
      ],
      "metadata": {
        "id": "DCAnvc-zg_9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate performance"
      ],
      "metadata": {
        "id": "7Gi0TDJ-hdDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best model according to the validation loss\n",
        "# (given that we use early stopping, this is not necessarily the last epoch)\n",
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
      ],
      "metadata": {
        "id": "vf-CTisThjyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte mean absolute error on validation set\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "(actuals - predictions).abs().mean()"
      ],
      "metadata": {
        "id": "Cuo1DKAYhtCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
        "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
      ],
      "metadata": {
        "id": "W6CeeKxFh4bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
      ],
      "metadata": {
        "id": "29A6DWoMh9QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Worst performers"
      ],
      "metadata": {
        "id": "gXNH-Ctyivjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte metric by which to display\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "mean_losses = SMAPE(reduction=\"none\")(predictions, actuals).mean(1)\n",
        "indices = mean_losses.argsort(descending=True)  # sort losses\n",
        "\n",
        "# Only show the worst performers\n",
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(\n",
        "        x, raw_predictions, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles)\n",
        "    );"
      ],
      "metadata": {
        "id": "Zjkk0dW_ic1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actuals vs predictions by variables"
      ],
      "metadata": {
        "id": "FWE8jiJ8iyry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, x = best_tft.predict(val_dataloader, return_x=True)\n",
        "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
        "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
      ],
      "metadata": {
        "id": "NrWKcqBYiz1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRediction"
      ],
      "metadata": {
        "id": "CwUThNRTj_xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict on selected data"
      ],
      "metadata": {
        "id": "WfcvPB4YkBqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_tft.predict(\n",
        "    training.filter(lambda x: (x.agency == \"Agency_01\") & (x.sku == \"SKU_01\") & (x.time_idx_first_prediction == 15)),\n",
        "    mode=\"quantiles\",\n",
        ")"
      ],
      "metadata": {
        "id": "NszgcapokJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_prediction, x = best_tft.predict(\n",
        "    training.filter(lambda x: (x.agency == \"Agency_01\") & (x.sku == \"SKU_01\") & (x.time_idx_first_prediction == 15)),\n",
        "    mode=\"raw\",\n",
        "    return_x=True,\n",
        ")\n",
        "best_tft.plot_prediction(x, raw_prediction, idx=0);"
      ],
      "metadata": {
        "id": "ULeclNLSkPzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict on new data\n",
        "\n",
        "Notice: cause we have covariates in the dataset, predicting on new data requires us to define the known covariates upfront."
      ],
      "metadata": {
        "id": "OFpDnLG1kYku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select last 24 months from data (max_encoder_length is 24)\n",
        "encoder_data = data[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n",
        "\n",
        "# select last known data point and create decoder data from it by repeating it and incrementing the month\n",
        "# in a real world dataset, we should not just forward fill the covariates but specify them to account\n",
        "# for changes in special days and prices (which you absolutely should do but we are too lazy here)\n",
        "last_data = data[lambda x: x.time_idx == x.time_idx.max()]\n",
        "decoder_data = pd.concat(\n",
        "    [last_data.assign(date=lambda x: x.date + pd.offsets.MonthBegin(i)) for i in range(1, max_prediction_length + 1)],\n",
        "    ignore_index=True,\n",
        ")\n",
        "\n",
        "# add time index consistent with \"data\"\n",
        "decoder_data[\"time_idx\"] = decoder_data[\"date\"].dt.year * 12 + decoder_data[\"date\"].dt.month\n",
        "decoder_data[\"time_idx\"] += encoder_data[\"time_idx\"].max() + 1 - decoder_data[\"time_idx\"].min()\n",
        "\n",
        "# adjust additional time feature(s)\n",
        "decoder_data[\"month\"] = decoder_data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
        "\n",
        "# combine encoder and decoder data\n",
        "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "QuTCJnVlkaRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
        "\n",
        "for idx in range(10):  # plot 10 examples\n",
        "    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);"
      ],
      "metadata": {
        "id": "2PTVUAlskmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interprete model"
      ],
      "metadata": {
        "id": "mOObGtsXkr7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
        "best_tft.plot_interpretation(interpretation)"
      ],
      "metadata": {
        "id": "dUFy-aKDkvcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TcTbflMqrOwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn (Bonus): Predicting stocks price with TFT (and make $$)"
      ],
      "metadata": {
        "id": "iQuw9tq3ugRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task: \n",
        "\n",
        "* Download some stock data\n",
        "* Train a TFT model\n",
        "* USe maybe several stocks that might be related at once\n",
        "* Other covariates possible?\n"
      ],
      "metadata": {
        "id": "XAqAzxWprfFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting data"
      ],
      "metadata": {
        "id": "zpO1Ps1PepKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install yfinance"
      ],
      "metadata": {
        "id": "My0C9TKf1MR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "Ri6A4fqKrBGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks = yf.download(tickers=['GOOGL'], period='10y', interval='1d') # , 'AAPL', 'GOOGL'"
      ],
      "metadata": {
        "id": "J5qqOZRb4L1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks.head()"
      ],
      "metadata": {
        "id": "Ayrm_S5WLiEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks.dtypes"
      ],
      "metadata": {
        "id": "QMqQrXjZZT73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stocks = df_stocks.drop('Volume', axis=1)"
      ],
      "metadata": {
        "id": "zE-cNAr_ZpHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.Chart(data = df_stocks.reset_index()).mark_line().encode(\n",
        "    x='Date:T',\n",
        "    y='Close:Q'\n",
        ")"
      ],
      "metadata": {
        "id": "HP603oTuLXUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ressources and cool stuff\n",
        "\n",
        "* [Timeseries Transformer on Huggingface](https://huggingface.co/docs/transformers/model_doc/time_series_transformer): The Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting."
      ],
      "metadata": {
        "id": "qvb_vsLTmxBq"
      }
    }
  ]
}