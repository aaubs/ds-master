{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Real-World Regression: Predicting Airbnb Prices in Copenhagen\n",
        "### From Messy Data to Interpretable Models with SHAP\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this hands-on walkthrough on real-world regression. Building on the foundational concepts of supervised learning, this session dives into a practical, end-to-end data science project. We will navigate the complexities of a messy, real-world dataset to build a predictive model that delivers tangible business value.\n",
        "\n",
        "Imagine you are a data scientist hired by a Copenhagen-based property investment firm. The firm wants to expand its portfolio of short-term rental properties but needs a data-driven approach to identify undervalued assets and optimize pricing strategies. Your task is to develop a model that can accurately predict the nightly rental price of a Copenhagen Airbnb listing based on its characteristics.\n",
        "\n",
        "This problem is a classic regression task. We want to learn a mapping function from property features (`X`) to a continuous price (`Y`). This can be formally expressed as:\n",
        "\n",
        "$$ \\text{Price} = f(\\text{location, amenities, host\\_characteristics}) + \\epsilon $$\n",
        "\n",
        "Where `f` is the unknown pricing function we aim to estimate, and `Œµ` represents market noise and unobserved factors. Our goal is to find an estimated function, `fÃÇ`, that generalizes well to new, unlisted properties.\n",
        "\n",
        "**üéØ Learning Objectives:**\n",
        "\n",
        "By the end of this walkthrough, you will be able to:\n",
        "\n",
        "1.  **Execute** an end-to-end data science workflow: from data acquisition and cleaning to model deployment and interpretation.\n",
        "2.  **Implement** robust feature engineering techniques for messy data types (text, geographic, temporal).\n",
        "3.  **Construct** a `scikit-learn` preprocessing pipeline to prevent data leakage and streamline modeling.\n",
        "4.  **Compare** the performance of various regression models, from simple linear baselines to powerful tree-based ensembles.\n",
        "5.  **Tune** model hyperparameters systematically using an optimization framework (`Optuna`).\n",
        "6.  **Interpret** complex models and explain individual predictions using SHAP (SHapley Additive exPlanations).\n",
        "7.  **Translate** model insights into actionable business recommendations for a real-world scenario.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Session Setup: Importing Libraries\n",
        "\n",
        "First, we load the necessary Python libraries. This project requires a broader toolkit than a simple introduction, including libraries for geographic calculations (`geopy`, `folium`), advanced modeling (`xgboost`), hyperparameter tuning (`optuna`), and model interpretability (`shap`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "# Scikit-learn for preprocessing, modeling, and evaluation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "# Advanced libraries\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import optuna\n",
        "import folium\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Settings for reproducibility and display\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set a professional plot style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"viridis\")\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. Data Acquisition & Initial Exploration\n",
        "\n",
        "Our first step is to acquire and understand the raw material for our model: the data. We will use publicly available data from [Inside Airbnb](http://insideairbnb.com/get-the-data/).\n",
        "\n",
        "### 1.1. The Business Problem\n",
        "\n",
        "The Copenhagen rental market is dynamic, influenced by high tourism, strict municipal regulations, and distinct neighborhood characteristics (from the historic Indre By to the trendy Vesterbro). Our model must capture these nuances to be effective. The goal is to predict the `price` per night for a given listing.\n",
        "\n",
        "### 1.2. Data Loading\n",
        "\n",
        "We'll download the detailed listings data directly into our environment. This ensures our analysis is reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data (this may take a minute)\n",
        "!wget -q https://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2025-06-27/data/listings.csv.gz -O listings.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# We specify low_memory=False to avoid DtypeWarning due to mixed types in some columns.\n",
        "df_raw = pd.read_csv('listings.csv.gz', compression='gzip', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a first impression of the data\n",
        "print(f\"Dataset shape: {df_raw.shape}\")\n",
        "print(f\"Memory usage: {df_raw.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
        "df_raw.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Initial Data Quality Assessment\n",
        "\n",
        "Real-world data is rarely clean. A quick look at the column types and non-null counts reveals a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output shows several challenges:\n",
        "- **Mixed Data Types:** Some columns that should be numeric are `object` type (e.g., `price`, `host_response_rate`).\n",
        "- **Widespread Missing Data:** Many columns like `bathrooms`, `bedrooms`, `review_scores_rating`, and most `host_*` columns are missing significant amounts of data. `neighbourhood_group_cleansed` and `license` are entirely empty.\n",
        "\n",
        "A heatmap can visualize the extent of missing data across the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_raw.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
        "plt.title(\"Missing Value Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üí° **Pro Tip:** Many columns are almost entirely empty or contain descriptive text not immediately useful for modeling. We'll need to be selective about which features to pursue.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Cleaning & Feature Engineering\n",
        "\n",
        "This is the most critical phase. The quality of our features will determine the performance ceiling of our model. We'll create a clean DataFrame `df` to work with.\n",
        "\n",
        "**üéØ Section Objectives:**\n",
        "\n",
        "-   Clean and transform the target variable (`price`).\n",
        "-   Parse and extract numerical data from text columns.\n",
        "-   Engineer new features from geographic and temporal data.\n",
        "\n",
        "### 2.1. Target Variable Cleaning (`price`)\n",
        "\n",
        "Our target variable `price` is an `object` type because it contains currency symbols (`$`) and commas. Although the currency is Danish Krone (DKK), it's formatted with a dollar sign. We must convert this into a clean numeric format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a copy to avoid modifying the original raw data\n",
        "df = df_raw.copy()\n",
        "\n",
        "# A more robust function to clean the price column\n",
        "def clean_price(price_series):\n",
        "    \"\"\"\n",
        "    Cleans a pandas Series of price strings.\n",
        "    - Handles NaN values gracefully.\n",
        "    - Removes currency symbols and commas.\n",
        "    - Converts the result to a numeric type, coercing errors.\n",
        "    \"\"\"\n",
        "    # Use the .str accessor which automatically skips NaN values\n",
        "    # The regex '[$,]' matches either a '$' or a ','\n",
        "    cleaned_series = price_series.str.replace(r'[$,]', '', regex=True)\n",
        "    \n",
        "    # Convert to numeric, turning any values that can't be converted into NaN\n",
        "    return pd.to_numeric(cleaned_series, errors='coerce')\n",
        "\n",
        "df['price_dkk'] = clean_price(df['price'])\n",
        "\n",
        "# Verify the result by checking the dtype and for any remaining NaNs\n",
        "print(\"Cleaned price column data type:\", df['price_dkk'].dtype)\n",
        "print(f\"Number of null prices after cleaning: {df['price_dkk'].isnull().sum()}\")\n",
        "df[['price', 'price_dkk']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Outlier Analysis\n",
        "\n",
        "Extreme price values (e.g., placeholder values like 0 or luxury listings costing tens of thousands) can skew our model. We'll use winsorization‚Äîcapping extreme values at a given percentile‚Äîto create a more stable target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Describe the price distribution before handling outliers\n",
        "print(\"Price distribution before outlier handling:\")\n",
        "print(df['price_dkk'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Winsorize: Cap prices at the 1st and 99th percentiles\n",
        "lower_bound = df['price_dkk'].quantile(0.01)\n",
        "upper_bound = df['price_dkk'].quantile(0.99)\n",
        "df['price_dkk'] = df['price_dkk'].clip(lower=lower_bound, upper=upper_bound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot before and after distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "sns.histplot(clean_price(df_raw['price']), ax=axes[0], bins=50, kde=True).set_title(\"Original Price Distribution (DKK)\")\n",
        "sns.histplot(df['price_dkk'], ax=axes[1], bins=50, kde=True).set_title(\"Cleaned Price Distribution (DKK)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Interpretation:** By cleaning the `price` data, we've created a more stable target variable. The original data had a maximum price over 70,000 DKK, which was likely an outlier. Our cleaned distribution is more focused on the core market, allowing our model to learn more generalizable patterns.\n",
        "\n",
        "### 2.2. Core Features Processing\n",
        "\n",
        "We will now process a selection of promising features.\n",
        "\n",
        "#### Bathroom Text Parsing\n",
        "\n",
        "The `bathrooms_text` column is descriptive (e.g., \"1.5 baths\", \"Half-bath\"). We can extract the numeric value using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_bathrooms(text_series):\n",
        "    \"\"\"\n",
        "    Parses a text series containing bathroom information into a numeric format.\n",
        "    Handles values like '1.5 baths', '1 shared bath', and 'Half-bath'.\n",
        "    \"\"\"\n",
        "    # Work with a copy to ensure we are modifying strings\n",
        "    s = text_series.copy().astype(str).str.lower()\n",
        "\n",
        "    # Step 1: Replace any string containing 'half-bath' with the numeric string '0.5'.\n",
        "    # This handles 'Half-bath', 'Private half-bath', 'Shared half-bath', etc.\n",
        "    s[s.str.contains('half-bath', na=False)] = '0.5'\n",
        "\n",
        "    # Step 2: Extract the first floating point number found in the remaining strings.\n",
        "    # The .str.extract() method returns a DataFrame, so we select the first column [0].\n",
        "    numeric_strings = s.str.extract(r'(\\d+\\.?\\d*)')[0]\n",
        "    \n",
        "    # Step 3: Convert the resulting series of strings to numbers.\n",
        "    # 'errors='coerce'' ensures that any non-numeric results become NaN.\n",
        "    return pd.to_numeric(numeric_strings, errors='coerce')\n",
        "\n",
        "df['bathrooms_numeric'] = parse_bathrooms(df['bathrooms_text'])\n",
        "\n",
        "# Let's run more comprehensive checks to be sure\n",
        "print(\"Parsing examples:\")\n",
        "print(f\"'1.5 baths' -> {parse_bathrooms(pd.Series(['1.5 baths'])).iloc[0]}\")\n",
        "print(f\"'Half-bath' -> {parse_bathrooms(pd.Series(['Half-bath'])).iloc[0]}\")\n",
        "print(f\"'2 shared baths' -> {parse_bathrooms(pd.Series(['2 shared baths'])).iloc[0]}\")\n",
        "\n",
        "# The original check should now pass, plus a new one for our special case\n",
        "assert df[df['bathrooms_text'] == '1.5 baths']['bathrooms_numeric'].iloc[0] == 1.5\n",
        "assert df[df['bathrooms_text'] == 'Shared half-bath']['bathrooms_numeric'].iloc[0] == 0.5\n",
        "\n",
        "print(\"\\nAssertions passed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Boolean Feature Conversion\n",
        "\n",
        "Columns like `host_is_superhost` and `instant_bookable` are stored as 't'/'f'. We convert them to a binary 0/1 format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['host_is_superhost'] = df['host_is_superhost'].map({'t': 1, 'f': 0})\n",
        "df['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Geographic Feature Engineering\n",
        "\n",
        "Location is paramount in real estate. We can create powerful features based on latitude and longitude.\n",
        "\n",
        "#### Distance to City Center\n",
        "\n",
        "Let's calculate the distance of each listing to a central landmark, R√•dhuspladsen (City Hall Square)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cph_center = (55.676098, 12.568337) # R√•dhuspladsen coordinates\n",
        "\n",
        "def calculate_distance_to_center(df_geo):\n",
        "    lat = pd.to_numeric(df_geo['latitude'], errors='coerce')\n",
        "    lon = pd.to_numeric(df_geo['longitude'], errors='coerce')\n",
        "    locations = list(zip(lat, lon))\n",
        "    distances = [\n",
        "        great_circle(loc, cph_center).kilometers if not (np.isnan(loc[0]) or np.isnan(loc[1])) else np.nan\n",
        "        for loc in locations\n",
        "    ]\n",
        "    return distances\n",
        "\n",
        "df['distance_to_center_km'] = calculate_distance_to_center(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interactive Map of Listings\n",
        "\n",
        "A `folium` map helps visualize the geographic distribution and price clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample to avoid overloading the map\n",
        "df_sample = df.sample(n=1000, random_state=42)\n",
        "\n",
        "# Create a map centered on Copenhagen\n",
        "map_cph = folium.Map(location=[55.6761, 12.5683], zoom_start=12)\n",
        "\n",
        "# Add points to the map\n",
        "for idx, row in df_sample.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        radius=3,\n",
        "        color='purple',\n",
        "        fill=True,\n",
        "        fill_color='purple',\n",
        "        fill_opacity=0.6,\n",
        "        popup=f\"Price: {row['price_dkk']:.0f} DKK\"\n",
        "    ).add_to(map_cph)\n",
        "\n",
        "# Display map. In some environments (like a standard script),\n",
        "# you might need to save it to a file: map_cph.save('copenhagen_listings.html')\n",
        "# or use map_cph.show_in_browser() to open in a new tab.\n",
        "# map_cph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Temporal Features\n",
        "\n",
        "How long a listing has been active can be an indicator of its quality and host experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert date columns to datetime objects\n",
        "df['first_review'] = pd.to_datetime(df['first_review'], errors='coerce')\n",
        "df['last_scraped'] = pd.to_datetime(df['last_scraped'], errors='coerce')\n",
        "\n",
        "# Calculate days since the first review\n",
        "# We use the latest scrape date as our 'today' for consistency\n",
        "latest_date = df['last_scraped'].max()\n",
        "df['days_since_first_review'] = (latest_date - df['first_review']).dt.days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚úÖ **Checkpoint:** We have now transformed messy, raw data into a structured format with clean, engineered features. Our dataset is much better prepared for modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Train-Test Split Strategy\n",
        "\n",
        "Before modeling, we must split our data. A simple random split is often sufficient, but for this dataset, we need to be thoughtful.\n",
        "\n",
        "### 3.1. Preventing Data Leakage\n",
        "\n",
        "A single host can have multiple listings. If we randomly split the data, we might have listings from the same host in both our training and testing sets. The model could inadvertently learn host-specific pricing patterns, leading to an overly optimistic performance estimate.\n",
        "\n",
        "‚ö†Ô∏è **Common Pitfall:** Ignoring grouped data structures (like hosts, patients, or stores) during splitting can lead to data leakage and models that fail in production. A more robust method is `GroupKFold`, which ensures all listings from a given host are in the same split. For this workshop, we'll proceed with a standard split but acknowledge this limitation.\n",
        "\n",
        "We will create three sets: training, validation (for hyperparameter tuning), and testing (for final, unbiased evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final feature selection for our model\n",
        "# We select a mix of numeric, categorical, and boolean features.\n",
        "numeric_features = [\n",
        "    'accommodates', 'bathrooms_numeric', 'bedrooms', 'beds',\n",
        "    'review_scores_rating', 'distance_to_center_km', 'days_since_first_review'\n",
        "]\n",
        "categorical_features = ['neighbourhood_cleansed', 'room_type', 'property_type']\n",
        "boolean_features = ['host_is_superhost', 'instant_bookable']\n",
        "\n",
        "# Combine all features\n",
        "all_features = numeric_features + categorical_features + boolean_features\n",
        "target = 'price_dkk'\n",
        "\n",
        "# Drop rows where our target or key features are missing\n",
        "df_model = df[all_features + [target]].dropna().copy()\n",
        "\n",
        "X = df_model[all_features]\n",
        "y = df_model[target]\n",
        "\n",
        "# Let's check the dtypes of our final feature set before splitting\n",
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First split: 70% train, 30% temp (for validation and test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Second split: Split the temp set into 50% validation, 50% test (15% of original each)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the shapes of our datasets\n",
        "print(f\"Training set shape:   {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")\n",
        "print(f\"Test set shape:       {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Preprocessing Pipeline\n",
        "\n",
        "We will use `scikit-learn`'s `Pipeline` and `ColumnTransformer` to create a reproducible preprocessing workflow. This is a cornerstone of production-ready machine learning.\n",
        "\n",
        "### 4.1. Why Use a Pipeline?\n",
        "\n",
        "A `scikit-learn` Pipeline is like a recipe for your data. It chains together multiple preprocessing and modeling steps into a single object. This has several major advantages:\n",
        "\n",
        "1.  **Preventing Data Leakage:** This is the most critical reason. When you perform operations like imputation (filling missing values) or scaling, you must learn the parameters (e.g., the median, the standard deviation) from the **training data only**. A pipeline ensures that when you call `.fit()` on the training data, these parameters are learned and stored. When you call `.transform()` on the validation or test data, it applies the *already learned* parameters, preventing any information from the test set from \"leaking\" into your training process.\n",
        "2.  **Reproducibility:** A pipeline encapsulates your entire workflow. Anyone (including your future self) can take your pipeline object and raw data and perfectly reproduce your results.\n",
        "3.  **Simplicity & Organization:** Instead of manually transforming each dataset (`X_train`, `X_val`, `X_test`), you define the steps once and let the pipeline handle the application, reducing code duplication and the risk of errors.\n",
        "\n",
        "### 4.2. Scikit-learn Pipeline Architecture\n",
        "\n",
        "Our pipeline will have separate branches for numeric and categorical features, assembled by a `ColumnTransformer`.\n",
        "\n",
        "1.  **Numeric Transformer:** Fills missing values (imputes) with the median and then scales the data to have zero mean and unit variance (`StandardScaler`).\n",
        "2.  **Categorical Transformer:** Fills missing values with a constant (\"missing\") and then converts categories into numerical format using one-hot encoding. `handle_unknown='ignore'` is crucial for dealing with new categories in the test set that weren't seen during training.\n",
        "3.  **Boolean Transformer:** Since we already converted these to 0/1, we can simply `'passthrough'` them without any changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the numeric pipeline\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Create the categorical pipeline\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3. Assembling the `ColumnTransformer`\n",
        "\n",
        "This object is the master chef that applies the correct recipe (transformer) to the correct ingredients (columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The ColumnTransformer applies specified transformers to columns of a DataFrame.\n",
        "# The structure is a list of tuples: (name, transformer_object, list_of_columns)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('bool', 'passthrough', boolean_features)\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified in our feature lists\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `preprocessor` is a powerful tool. When we call `.fit(X_train)`, it learns the median, standard deviation, and categories *only from the training data*. When we later call `.transform()`, it applies these learned parameters to any new data we provide.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Model Development\n",
        "\n",
        "With our data prepared, we can now train and compare several regression models.\n",
        "\n",
        "### 5.1. Baseline Model\n",
        "\n",
        "We always start with a simple baseline. A `DummyRegressor` that always predicts the mean of the training target gives us a benchmark. Any real model must perform better than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the full pipeline with the preprocessor and a dummy model\n",
        "dummy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('regressor', DummyRegressor(strategy='mean'))])\n",
        "\n",
        "# Train and evaluate the baseline\n",
        "dummy_pipeline.fit(X_train, y_train)\n",
        "y_pred_dummy = dummy_pipeline.predict(X_val)\n",
        "rmse_dummy = np.sqrt(mean_squared_error(y_val, y_pred_dummy))\n",
        "\n",
        "print(f\"Baseline (Mean Predictor) RMSE: {rmse_dummy:,.2f} DKK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Interpretation:** The baseline model is, on average, wrong by about 739 DKK. This is our starting point. A successful model must significantly reduce this error.\n",
        "\n",
        "### 5.2. Linear Models\n",
        "\n",
        "Linear models are fast, interpretable, and provide a great second baseline. We'll try Ridge regression, which adds a penalty for large coefficients to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Ridge regression pipeline\n",
        "ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                 ('regressor', RidgeCV(alphas=np.logspace(-3, 3, 7)))])\n",
        "\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge_pipeline.predict(X_val)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_val, y_pred_ridge))\n",
        "\n",
        "print(f\"Ridge Regression RMSE: {rmse_ridge:,.2f} DKK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3. Tree-Based Models\n",
        "\n",
        "Tree-based ensembles like Random Forest and XGBoost are typically more powerful as they can capture non-linear relationships and interactions between features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Pipeline\n",
        "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))])\n",
        "\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "y_pred_rf = rf_pipeline.predict(X_val)\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "\n",
        "print(f\"Random Forest RMSE: {rmse_rf:,.2f} DKK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Pipeline\n",
        "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('regressor', xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1))])\n",
        "\n",
        "xgb_pipeline.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_pipeline.predict(X_val)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "\n",
        "print(f\"XGBoost RMSE: {rmse_xgb:,.2f} DKK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Interpretation:** Both Random Forest and XGBoost significantly outperform the linear model, reducing the average prediction error to around 500 DKK. This suggests that the relationship between property features and price is non-linear.\n",
        "\n",
        "### 5.4. Hyperparameter Tuning with Optuna\n",
        "\n",
        "Our Random Forest model performs well, but we used default settings. We can likely improve it by tuning its hyperparameters using `Optuna`.\n",
        "\n",
        "ü§î **Discussion Question:** What are hyperparameters, and why is it important to tune them using a separate validation set instead of the test set?\n",
        "\n",
        "::: {.callout-note collapse=\"true\" title=\"Click for a possible answer\"}\n",
        "Hyperparameters are settings for a learning algorithm that are not learned from the data itself (e.g., the number of trees in a random forest). They are set by the data scientist before training begins.\n",
        "\n",
        "It's crucial to use a validation set for tuning to avoid data leakage. If we tune on the test set, we are essentially \"peeking\" at the final evaluation data and choosing the parameters that work best for it. This leads to an artificially inflated performance score. The test set must be kept completely separate until the very end for a truly unbiased assessment of the final model's generalization ability.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the objective function for Optuna\n",
        "# We pass the pre-split data to the function\n",
        "def objective(trial, X_train, y_train, X_val, y_val):\n",
        "    # Fit the preprocessor on the training data ONLY\n",
        "    preprocessor.fit(X_train)\n",
        "    X_train_transformed = preprocessor.transform(X_train)\n",
        "    X_val_transformed = preprocessor.transform(X_val)\n",
        "    \n",
        "    # Define the search space for hyperparameters\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "    \n",
        "    # Create and train the model\n",
        "    model = RandomForestRegressor(**params)\n",
        "    model.fit(X_train_transformed, y_train)\n",
        "    \n",
        "    # Evaluate on the validation set\n",
        "    y_pred = model.predict(X_val_transformed)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    \n",
        "    return rmse\n",
        "\n",
        "# Create a study and optimize (run a small number of trials for the workshop)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "# Note: For a real project, n_trials would be much larger (e.g., 50-100).\n",
        "study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=20)\n",
        "\n",
        "print(f\"Best trial RMSE: {study.best_value:.2f} DKK\")\n",
        "print(\"Best hyperparameters:\", study.best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Model Evaluation\n",
        "\n",
        "Now we select our best model (the tuned Random Forest), train it on the combined training and validation data, and evaluate its final performance on the unseen test set.\n",
        "\n",
        "### 6.1. Final Model Training and Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the best hyperparameters from the Optuna study\n",
        "best_params = study.best_params\n",
        "best_params['random_state'] = 42\n",
        "best_params['n_jobs'] = -1\n",
        "\n",
        "# Create the final, optimized pipeline\n",
        "final_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                       ('regressor', RandomForestRegressor(**best_params))])\n",
        "\n",
        "# Combine train and validation sets for final training\n",
        "X_train_full = pd.concat([X_train, X_val])\n",
        "y_train_full = pd.concat([y_train, y_val])\n",
        "\n",
        "# Train on the full training data\n",
        "final_model_pipeline.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Evaluate on the unseen test set\n",
        "y_pred_final = final_model_pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate final performance metrics\n",
        "rmse_final = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
        "mape_final = mean_absolute_percentage_error(y_test, y_pred_final)\n",
        "r2_final = r2_score(y_test, y_pred_final)\n",
        "\n",
        "print(f\"Final Model Performance on Test Set:\")\n",
        "print(f\"RMSE: {rmse_final:,.2f} DKK\")\n",
        "print(f\"MAPE: {mape_final:.2%}\")\n",
        "print(f\"R¬≤ Score: {r2_final:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Interpretation:** Our final model has an average error of approximately 480 DKK. The MAPE of ~27% means our predictions are, on average, 27% off from the actual price. The R¬≤ score indicates that our model explains about 67% of the variance in Airbnb prices, a respectable result for such a complex market.\n",
        "\n",
        "### 6.2. Residual Analysis\n",
        "\n",
        "A residual plot helps diagnose model bias. We look for a random scatter around the zero line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals = y_test - y_pred_final\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(x=y_pred_final, y=residuals, alpha=0.5)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residual Plot (Predicted vs. Residuals)\")\n",
        "plt.xlabel(\"Predicted Price (DKK)\")\n",
        "plt.ylabel(\"Residuals (Actual - Predicted) (DKK)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot shows some heteroscedasticity (the variance of errors increases as the predicted price increases). Our model is better at predicting prices for cheaper listings than for expensive ones. This is a common finding in price prediction models.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Model Interpretability with SHAP\n",
        "\n",
        "Our Random Forest model is a \"black box.\" We know it performs well, but we don't know *why*. SHAP (SHapley Additive exPlanations) is a powerful technique to look inside the box.\n",
        "\n",
        "### 7.1. Global Explanations\n",
        "\n",
        "First, we need the preprocessed test data and the correct feature names to pass to SHAP. Extracting feature names from a `ColumnTransformer` can be tricky, so we'll break it down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Get the fitted preprocessor from the final pipeline\n",
        "fitted_preprocessor = final_model_pipeline.named_steps['preprocessor']\n",
        "\n",
        "# 2. Transform the test data\n",
        "X_test_transformed = fitted_preprocessor.transform(X_test)\n",
        "\n",
        "# 3. Extract feature names after one-hot encoding\n",
        "# Access the 'cat' transformer, then its 'onehot' step, then get the feature names\n",
        "cat_features_out = fitted_preprocessor.named_transformers_['cat']\\\n",
        "    .named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "\n",
        "# 4. Combine all feature names in the correct order\n",
        "feature_names = numeric_features + list(cat_features_out) + boolean_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a SHAP explainer for tree-based models\n",
        "explainer = shap.TreeExplainer(final_model_pipeline.named_steps['regressor'])\n",
        "shap_values = explainer.shap_values(X_test_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A summary plot shows the most important features and their impact. Each dot is a single prediction from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the summary plot\n",
        "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, max_display=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to Read This Plot\n",
        "\n",
        "*   **Features (Y-axis):** The features are listed on the left, ranked by their overall importance to the model's predictions, from most important (`bedrooms`) at the top to least important at the bottom.\n",
        "*   **Impact on Prediction (X-axis):** The horizontal axis shows the SHAP value. A positive value means that feature pushed the price prediction *higher*, while a negative value pushed it *lower*.\n",
        "*   **Feature Value (Color):** The color of each dot represents the value of that feature for a given listing. **Red dots are high values** (e.g., many bedrooms, far from the city center), and **blue dots are low values** (e.g., few bedrooms, close to the city center).\n",
        "*   **Each Dot:** Each dot represents a single prediction (a single listing) from your test set.\n",
        "\n",
        "\n",
        "### 7.2. Local Explanations\n",
        "\n",
        "SHAP can also explain individual predictions. Let's find an expensive listing and see why the model priced it that way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find an expensive listing in the test set\n",
        "expensive_listing_idx = np.argmax(y_test.values)\n",
        "expensive_listing_transformed = X_test_transformed[expensive_listing_idx]\n",
        "expensive_listing_original = X_test.iloc[[expensive_listing_idx]]\n",
        "\n",
        "print(f\"Explaining prediction for listing with actual price: {y_test.iloc[expensive_listing_idx]:,.2f} DKK\")\n",
        "print(f\"Model prediction: {y_pred_final[expensive_listing_idx]:,.2f} DKK\")\n",
        "expensive_listing_original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a force plot for this single prediction\n",
        "# This interactive plot shows features pushing the prediction higher (red) or lower (blue)\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[expensive_listing_idx, :], \n",
        "                X_test_transformed[expensive_listing_idx, :], feature_names=feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Interpretation:** The force plot shows how each feature \"pushes\" the prediction away from the baseline average price (`explainer.expected_value`). For this expensive listing, features like having 2 bathrooms, accommodating 6 people, and being an \"Entire home/apt\" are the primary drivers pushing the price up, while its distance from the center slightly pushes it down.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Business Recommendations\n",
        "\n",
        "Our model is not just a technical artifact; it's a tool for decision-making.\n",
        "\n",
        "### 8.1. Key Insights\n",
        "\n",
        "1.  **Core Amenities Drive Value:** The number of bathrooms, bedrooms, and accommodates are the most critical price drivers. Investments in adding or upgrading these facilities are likely to have the highest ROI.\n",
        "2.  **Location is Non-Negotiable:** Proximity to the city center is a massive price factor. For listings further out, highlighting other features (e.g., space, quiet) is crucial. Neighborhoods like Indre By and Vesterbro command a significant premium.\n",
        "3.  **Property Type Matters:** \"Entire home/apt\" listings are priced significantly higher than private or shared rooms. This model can help quantify that premium.\n",
        "\n",
        "### 8.2. Deployment Considerations\n",
        "\n",
        "-   **Pricing Tool:** The model can be deployed as an internal tool for portfolio managers to get a baseline price estimate for a potential new property.\n",
        "-   **Dynamic Pricing:** The model could be enhanced with seasonality data (e.g., holidays, events) and integrated into a dynamic pricing engine.\n",
        "-   **Monitoring:** The Copenhagen market changes. The model must be retrained periodically (e.g., quarterly) on new data, and its performance must be monitored for drift.\n",
        "\n",
        "### 8.3. Limitations & Future Work\n",
        "\n",
        "-   **Amenities:** We only used a few features. A more detailed parsing of the `amenities` column could uncover value drivers like \"hot tub\" or \"balcony\".\n",
        "-   **Image Data:** We ignored property photos, which contain a wealth of information. A computer vision model could be added to analyze image quality and features.\n",
        "-   **Seasonality:** Our model doesn't capture seasonal price fluctuations. This would require time-series data.\n",
        "\n",
        "---\n",
        "\n",
        "### Group Exercise\n",
        "\n",
        "**Tasks for Breakout Groups:**\n",
        "\n",
        "1.  **Feature Engineering Challenge:** The `amenities` column is a string representation of a list of features. Brainstorm and implement a strategy to parse this column. Create binary features for at least three amenities you believe would be important (e.g., `has_wifi`, `has_kitchen`, `has_washer`). Add one of these to the model pipeline and see if it improves the RMSE.\n",
        "\n",
        "2.  **Error Analysis:** Our model performs worse on expensive listings. Filter the test set to only include the top 10% most expensive properties.\n",
        "    -   Calculate the RMSE for just this segment. How does it compare to the overall RMSE?\n",
        "    -   Look at the SHAP summary plot for just this segment. Are the key price drivers different for luxury properties?\n",
        "\n",
        "3.  **Business Strategy Simulation:** Your firm has identified a property in the *N√∏rrebro* neighborhood. It has 2 bedrooms, 1 bathroom, and accommodates 4 people. Assume it's an \"Entire home/apt\" of type \"Entire rental unit\", has a superhost, is instantly bookable, has a review score of 4.8, is 2.5 km from the city center, and has been reviewed for 500 days. Create a DataFrame with this information and use your final trained model (`final_model_pipeline.predict()`) to estimate its nightly price. The seller is asking for a price that assumes a nightly rate of 1500 DKK. Based on your model's prediction, would you advise the firm that this is a potentially undervalued, fairly valued, or overvalued investment? What other information would you want?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/opt/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
