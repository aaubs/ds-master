{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_4_CNN_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Convolutional Neural Networks (CNNs)\n"
      ],
      "metadata": {
        "id": "MFR0zA4fN9A0"
      },
      "id": "MFR0zA4fN9A0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for tasks related to computer vision, such as image classification, object detection, and image segmentation. CNNs have proven to be highly effective in these tasks because they can automatically learn hierarchical features from raw pixel data. Let's dive into the key components of CNNs:\n",
        "\n"
      ],
      "metadata": {
        "id": "OrjELDf4OApb"
      },
      "id": "OrjELDf4OApb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3ff482",
      "metadata": {
        "id": "ec3ff482"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Embeddings\n",
        "\n",
        "Image embeddings are a representation of images in the form of high-dimensional vectors of numerical values. This technique transforms raw images into a form that a computer can understand and process. The goal of creating image embeddings is to capture the essential features of an image, such as shapes, colors, textures, or any other relevant visual information, in a compact numerical format."
      ],
      "metadata": {
        "id": "Jo3jmglbSjbL"
      },
      "id": "Jo3jmglbSjbL"
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "\n",
        "<iframe width=\"428\" height=\"761\" src=\"https://www.youtube.com/embed/TJOLwBGq9bU\" title=\"Leo Messi Dice Art Timelapse\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "u8LaB38aRt4m",
        "outputId": "cba7bedd-335e-4c93-b665-cf2db48dcca6"
      },
      "id": "u8LaB38aRt4m",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"428\" height=\"761\" src=\"https://www.youtube.com/embed/TJOLwBGq9bU\" title=\"Leo Messi Dice Art Timelapse\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image embeddings generated by models, especially those involving deep learning techniques like CNNs, are adept at capturing a wide range of information from images, including edges, corners, light intensity, textures, and other visual features. This capability stems from the hierarchical nature of how these models process image data.\n",
        "\n",
        "In the early layers of a CNN, the model learns to identify simple features such as edges and corners. These are basic patterns that are fundamental to the structure of objects within images. As the information passes through subsequent layers, the network combines these simple features to identify more complex patterns, such as textures, shapes, and eventually, entire objects or scenes."
      ],
      "metadata": {
        "id": "uGwpjCfXUZoX"
      },
      "id": "uGwpjCfXUZoX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-16-10-58-29.png)"
      ],
      "metadata": {
        "id": "wx6M2HSkcvE1"
      },
      "id": "wx6M2HSkcvE1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "transforms.Normalize(mean, std): After converting the image to a tensor, this transformation normalizes the tensor's values to have a mean of 0 and a standard deviation of 1, which helps in training machine learning models. In the code snippet, it's subtracting the mean (0.5, 0.5, 0.5) from each channel (R, G, B) and then dividing by the standard deviation (0.5, 0.5, 0.5) for each channel. This operation scales the pixel values to be in the range [-1, 1] for each channel"
      ],
      "metadata": {
        "id": "OPiZdkbQYiBx"
      },
      "id": "OPiZdkbQYiBx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-16-11-01-53.png)"
      ],
      "metadata": {
        "id": "BiOyGfOpcyvu"
      },
      "id": "BiOyGfOpcyvu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-16-11-03-58.png)"
      ],
      "metadata": {
        "id": "R9Al6qiFc2GC"
      },
      "id": "R9Al6qiFc2GC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909a5621",
      "metadata": {
        "id": "909a5621"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters of the model\n",
        "num_epochs = 4\n",
        "batch_size = 16\n",
        "learning_rate = 0.003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11f9490",
      "metadata": {
        "id": "e11f9490"
      },
      "outputs": [],
      "source": [
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Dataset Overview\n",
        "\n",
        "The CIFAR-10 dataset is a widely used dataset in the field of computer vision and machine learning. It is commonly used for tasks such as image classification and object recognition. Here are some key details about the CIFAR-10 dataset:\n",
        "\n",
        "## Dataset Details\n",
        "\n",
        "- **Number of Classes:** CIFAR-10 consists of 10 classes or categories, each representing a different object or category commonly found in everyday life.\n",
        "\n",
        "- **Images:** The dataset contains a total of 60,000 images, with 6,000 images per class. These images are divided into two subsets:\n",
        "  - **Training Set:** This subset contains 50,000 images, with 5,000 images per class. It is used for training machine learning models.\n",
        "  - **Test Set:** The remaining 10,000 images, with 1,000 images per class, make up the test set. This set is used for evaluating the performance of trained models.\n",
        "\n",
        "- **Image Size:** Each image in the CIFAR-10 dataset is a color image with dimensions 32 pixels in height and 32 pixels in width. Therefore, the images have a resolution of 32x32 pixels."
      ],
      "metadata": {
        "id": "UyyKH4l6YF1Y"
      },
      "id": "UyyKH4l6YF1Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://sichkar-valentyn.github.io/cifar10/images/CIFAR-10_examples.png)"
      ],
      "metadata": {
        "id": "q10DyELDSUem"
      },
      "id": "q10DyELDSUem"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0616bc72",
      "metadata": {
        "id": "0616bc72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58cc046-4bb4-4063-9c70-bd31362b6786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 98177520.73it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data_CIFAR10/cifar-10-python.tar.gz to ./data_CIFAR10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#DOWNLOAD DATASET\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data_CIFAR10', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data_CIFAR10', train=False,\n",
        "                                            download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e4ca5e",
      "metadata": {
        "id": "a2e4ca5e"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main components in the architecture of a CNN model"
      ],
      "metadata": {
        "id": "WzuwaOF_YlHw"
      },
      "id": "WzuwaOF_YlHw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://editor.analyticsvidhya.com/uploads/34881cnn_architecture_1.png)"
      ],
      "metadata": {
        "id": "PkylUcXHY9Hk"
      },
      "id": "PkylUcXHY9Hk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Dataset Overview\n",
        "\n",
        "The CIFAR-10 dataset is a widely used dataset in the field of computer vision and machine learning, specifically designed for image classification tasks. It focuses on classifying images into one of ten distinct categories. Below are the primary components of a Convolutional Neural Network (CNN) when applied to the CIFAR-10 dataset:\n",
        "\n",
        "## 1. Convolutional Layer\n",
        "\n",
        "Convolutional Layers are fundamental for extracting meaningful features from images in the CIFAR-10 dataset. They employ filters that slide over the image grid to identify relevant patterns. Let's delve into this process:\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/36813convolution_overview.gif)\n",
        "\n",
        "In this illustration, a window, represented by a convolutional filter, moves across the entire image.\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/89792convolution_example.png)\n",
        "\n",
        "During this operation, each filter element multiplies with the corresponding image element, and the products are summed to generate a single value in the output feature map. This process continues until the entire input feature map is covered, resulting in the populated output feature map.\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/578272021-07-20%2023_09_31-ML%20Practicum_%20Image%20Classification%20%C2%A0_%C2%A0%20Google%20Developers.png)\n",
        "\n",
        "## 2. Pooling Layer\n",
        "\n",
        "Pooling Layers are utilized to reduce the dimensionality of feature maps. Here, a 2x2 max-pooling layer is used. As the window traverses the image, it selects the maximum value within the window:\n",
        "\n",
        "![Maxpool Layer](https://editor.analyticsvidhya.com/uploads/66402maxpool_animation.gif)\n",
        "\n",
        "Following the max-pooling operation, the input's dimension, originally 4x4, is downsized to 2x2. This dimension reduction is essential for managing computational complexity and enhancing feature recognition.\n",
        "\n",
        "## 3. Fully Connected Layer\n",
        "\n",
        "The Fully Connected Layer serves as the final section of the CNN architecture. It receives the rich features extracted from the CIFAR-10 dataset using convolutional filters. These features are then forwarded through the network, ultimately reaching the output layer. In the output layer, the model predicts the probability of the input image belonging to one of the ten predefined classes. The final predicted output corresponds to the class with the highest probability according to the model's prediction.\n",
        "\n",
        "In summary, this explanation provides an overview of the primary components of a CNN architecture when applied to the CIFAR-10 dataset. Convolutional Layers extract features, Pooling Layers reduce dimensionality, and Fully Connected Layers make the final class predictions. This understanding forms the foundation for implementing the CNN architecture in code.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0fOePkVZDQ-"
      },
      "id": "Y0fOePkVZDQ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f0b4db",
      "metadata": {
        "id": "a8f0b4db"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 1. Creating a Neural Network\n",
        "# Define the ConvNet using nn.Sequential\n",
        "conv_net = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 5),  # Convolutional layer 1: 3 input channels, 6 filters, 5x5 filter size\n",
        "    nn.ReLU(),          # ReLU activation function\n",
        "    nn.MaxPool2d(2, 2), # Max pooling layer: size 2x2, stride 2\n",
        "\n",
        "    nn.Conv2d(6, 16, 5), # Convolutional layer 2: 6 input channels, 16 filters, 5x5 filter size\n",
        "    nn.ReLU(),          # ReLU activation function\n",
        "    nn.MaxPool2d(2, 2), # Max pooling layer: size 2x2, stride 2\n",
        "\n",
        "    nn.Flatten(),       # Flatten the output for fully connected layers\n",
        "\n",
        "    nn.Linear(16*5*5, 120), # Fully connected layer 1\n",
        "    nn.ReLU(),              # ReLU activation function\n",
        "\n",
        "    nn.Linear(120, 84),     # Fully connected layer 2\n",
        "    nn.ReLU(),              # ReLU activation function\n",
        "\n",
        "    nn.Linear(84, 10)       # Fully connected layer 3 (output layer)\n",
        ")\n",
        "\n",
        "# Define the device (CPU or GPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of the ConvNet model and move it to the specified device\n",
        "model = conv_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/car.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define the same transform as used for your training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize the image to 32x32 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
        "])\n",
        "\n",
        "# Apply the transform to the image\n",
        "image_transformed = transform(image).float()\n",
        "\n",
        "# Add an extra batch dimension since PyTorch treats all inputs as batches\n",
        "image_transformed = image_transformed.unsqueeze(0)\n",
        "\n",
        "# Check the image tensor shape to ensure it's ready for the model (should be [1, 3, 32, 32])\n",
        "print(image_transformed.shape)"
      ],
      "metadata": {
        "id": "07pOfKjRZMNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb6e32a-8429-4daf-8eb3-c0b09fc7a989"
      },
      "id": "07pOfKjRZMNs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `net` is your trained model. Ensure it's in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Make a prediction\n",
        "output = model(image_transformed)\n",
        "\n",
        "# Get the index of the highest score in the output\n",
        "_, predicted = torch.max(output.data, 1)\n",
        "\n",
        "print(f'Predicted Class: {classes[predicted.item()]}')"
      ],
      "metadata": {
        "id": "_9i-wJX1Yr-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33717197-f4ab-4b8b-86b3-812c52ee9008"
      },
      "id": "_9i-wJX1Yr-I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: frog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numChannels: The number of channels in the input images (1 for grayscale or 3 for RGB)"
      ],
      "metadata": {
        "id": "qLBonSZ1Yza3"
      },
      "id": "qLBonSZ1Yza3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa5cea5",
      "metadata": {
        "id": "1fa5cea5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss already includes SoftMax\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b1b6c4",
      "metadata": {
        "id": "c2b1b6c4",
        "outputId": "741efc24-b233-459f-819f-9de5f4ae79ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [2000/3125], Loss: 2.2898\n",
            "Epoch [2/4], Step [2000/3125], Loss: 1.9791\n",
            "Epoch [3/4], Step [2000/3125], Loss: 1.3779\n",
            "Epoch [4/4], Step [2000/3125], Loss: 1.6012\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 2. Forward Pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # 3. FeedForward Evaluation\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 4. Backward Pass / Gradient Calculation\n",
        "        optimizer.zero_grad() #with zero_grad() we ensure that the gradients are properly reset to zero at the start of each iteration\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Back Propagation / Update Weights\n",
        "        optimizer.step()\n",
        "        #wandb.log({\"loss\": loss})  uncomment this line if you want to send data to weights and biases interface\n",
        "\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "            # Calculate average accuracy for every 2000 steps\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for images, labels in test_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                accuracy = correct / total\n",
        "                #wandb.log({\"accuracy\": accuracy}) uncomment this line if you want to send data to weights and biases interface\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(labels.size(0)):  # Use labels.size(0) to get the batch size\n",
        "            label = labels[i].item()  # Extract the actual label value using .item()\n",
        "            pred = predicted[i].item()  # Extract the predicted label value using .item()\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        if n_class_samples[i] == 0:\n",
        "            acc = 0.0  # To avoid division by zero\n",
        "        else:\n",
        "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ3uBVHTPbJx",
        "outputId": "3ec1a7c4-5f7f-4a62-f297-3d8ebc122efa"
      },
      "id": "mJ3uBVHTPbJx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 41.48 %\n",
            "Accuracy of plane: 44.1 %\n",
            "Accuracy of car: 83.1 %\n",
            "Accuracy of bird: 11.0 %\n",
            "Accuracy of cat: 30.5 %\n",
            "Accuracy of deer: 36.4 %\n",
            "Accuracy of dog: 45.3 %\n",
            "Accuracy of frog: 55.4 %\n",
            "Accuracy of horse: 48.9 %\n",
            "Accuracy of ship: 31.9 %\n",
            "Accuracy of truck: 28.2 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/car.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define the same transform as used for your training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize the image to 32x32 pixels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
        "])\n",
        "\n",
        "# Apply the transform to the image\n",
        "image_transformed = transform(image).float()\n",
        "\n",
        "# Add an extra batch dimension since PyTorch treats all inputs as batches\n",
        "image_transformed = image_transformed.unsqueeze(0)\n",
        "\n",
        "# Check the image tensor shape to ensure it's ready for the model (should be [1, 3, 32, 32])\n",
        "print(image_transformed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg_0MMYWVq2L",
        "outputId": "7a3c2b3b-9419-43de-8801-cb61d2405823"
      },
      "id": "Wg_0MMYWVq2L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `net` is your trained model. Ensure it's in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Make a prediction\n",
        "output = model(image_transformed)\n",
        "\n",
        "# Get the index of the highest score in the output\n",
        "_, predicted = torch.max(output.data, 1)\n",
        "\n",
        "print(f'Predicted Class: {classes[predicted.item()]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3jeN7biV9Rx",
        "outputId": "80737c3b-c3d7-464e-f654-3b73bba88510"
      },
      "id": "E3jeN7biV9Rx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Image Classification with CNNs on Fashion-MNIST\n",
        "\n",
        "####**Objective:**\n",
        "The goal of this exercise is to build, train, and evaluate a Convolutional Neural Network (CNN) on the Fashion-MNIST dataset using PyTorch.\n",
        "\n",
        "####**Dataset:**\n",
        "Fashion-MNIST consists of 60,000 training images and 10,000 test images, each a 28x28 grayscale image associated with one of 10 fashion categories."
      ],
      "metadata": {
        "id": "TZhcbq5AZToK"
      },
      "id": "TZhcbq5AZToK"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Transformations applied on each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Loading the training dataset\n",
        "trainset_fashion_full = torchvision.datasets.FashionMNIST(root='./data_Fashion_MINST', train=True, download=True, transform=transform)\n",
        "indices = range(0, 60000) # 0 to 9999\n",
        "trainset_fashion = Subset(trainset_fashion_full, indices)\n",
        "trainloader_fashion = torch.utils.data.DataLoader(trainset_fashion, shuffle=True)\n",
        "\n",
        "# Loading the testing dataset\n",
        "testset_fashion = torchvision.datasets.FashionMNIST(root='./data_Fashion_MINST', train=False, download=True, transform=transform)\n",
        "testloader_fashion = torch.utils.data.DataLoader(testset_fashion, shuffle=False)\n",
        "\n",
        "# Classes in Fashion-MNIST\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ],
      "metadata": {
        "id": "q2R-PbDIWhQi"
      },
      "id": "q2R-PbDIWhQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/Fashion-MNIST-dataset.png)"
      ],
      "metadata": {
        "id": "RXRcyhffcDAF"
      },
      "id": "RXRcyhffcDAF"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZaXK68KIY0g1"
      },
      "id": "ZaXK68KIY0g1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}