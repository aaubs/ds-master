{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_4_CNN_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Convolutional Neural Networks (CNNs)\n"
      ],
      "metadata": {
        "id": "MFR0zA4fN9A0"
      },
      "id": "MFR0zA4fN9A0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for tasks related to computer vision, such as image classification, object detection, and image segmentation. CNNs have proven to be highly effective in these tasks because they can automatically learn hierarchical features from raw pixel data. Let's dive into the key components of CNNs:\n",
        "\n"
      ],
      "metadata": {
        "id": "OrjELDf4OApb"
      },
      "id": "OrjELDf4OApb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec3ff482",
      "metadata": {
        "id": "ec3ff482"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909a5621",
      "metadata": {
        "id": "909a5621"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters of the model\n",
        "num_epochs = 4\n",
        "batch_size = 16\n",
        "learning_rate = 0.003"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transforms.Normalize(mean, std): After converting the image to a tensor, this transformation normalizes the tensor's values to have a mean of 0 and a standard deviation of 1, which helps in training machine learning models. In the code snippet, it's subtracting the mean (0.5, 0.5, 0.5) from each channel (R, G, B) and then dividing by the standard deviation (0.5, 0.5, 0.5) for each channel. This operation scales the pixel values to be in the range [-1, 1] for each channel"
      ],
      "metadata": {
        "id": "OPiZdkbQYiBx"
      },
      "id": "OPiZdkbQYiBx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11f9490",
      "metadata": {
        "id": "e11f9490"
      },
      "outputs": [],
      "source": [
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Dataset Overview\n",
        "\n",
        "The CIFAR-10 dataset is a widely used dataset in the field of computer vision and machine learning. It is commonly used for tasks such as image classification and object recognition. Here are some key details about the CIFAR-10 dataset:\n",
        "\n",
        "## Dataset Details\n",
        "\n",
        "- **Number of Classes:** CIFAR-10 consists of 10 classes or categories, each representing a different object or category commonly found in everyday life.\n",
        "\n",
        "- **Images:** The dataset contains a total of 60,000 images, with 6,000 images per class. These images are divided into two subsets:\n",
        "  - **Training Set:** This subset contains 50,000 images, with 5,000 images per class. It is used for training machine learning models.\n",
        "  - **Test Set:** The remaining 10,000 images, with 1,000 images per class, make up the test set. This set is used for evaluating the performance of trained models.\n",
        "\n",
        "- **Image Size:** Each image in the CIFAR-10 dataset is a color image with dimensions 32 pixels in height and 32 pixels in width. Therefore, the images have a resolution of 32x32 pixels."
      ],
      "metadata": {
        "id": "UyyKH4l6YF1Y"
      },
      "id": "UyyKH4l6YF1Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://sichkar-valentyn.github.io/cifar10/images/CIFAR-10_examples.png)"
      ],
      "metadata": {
        "id": "q10DyELDSUem"
      },
      "id": "q10DyELDSUem"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0616bc72",
      "metadata": {
        "id": "0616bc72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34151fd3-31f4-4a6c-de98-e174df84d670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data_CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 63829407.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data_CIFAR10/cifar-10-python.tar.gz to ./data_CIFAR10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#DOWNLOAD DATASET\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data_CIFAR10', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data_CIFAR10', train=False,\n",
        "                                            download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e4ca5e",
      "metadata": {
        "id": "a2e4ca5e"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main components in the architecture of a CNN model"
      ],
      "metadata": {
        "id": "WzuwaOF_YlHw"
      },
      "id": "WzuwaOF_YlHw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://editor.analyticsvidhya.com/uploads/34881cnn_architecture_1.png)"
      ],
      "metadata": {
        "id": "PkylUcXHY9Hk"
      },
      "id": "PkylUcXHY9Hk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Dataset Overview\n",
        "\n",
        "The CIFAR-10 dataset is a widely used dataset in the field of computer vision and machine learning, specifically designed for image classification tasks. It focuses on classifying images into one of ten distinct categories. Below are the primary components of a Convolutional Neural Network (CNN) when applied to the CIFAR-10 dataset:\n",
        "\n",
        "## 1. Convolutional Layer\n",
        "\n",
        "Convolutional Layers are fundamental for extracting meaningful features from images in the CIFAR-10 dataset. They employ filters that slide over the image grid to identify relevant patterns. Let's delve into this process:\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/36813convolution_overview.gif)\n",
        "\n",
        "In this illustration, a window, represented by a convolutional filter, moves across the entire image.\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/89792convolution_example.png)\n",
        "\n",
        "During this operation, each filter element multiplies with the corresponding image element, and the products are summed to generate a single value in the output feature map. This process continues until the entire input feature map is covered, resulting in the populated output feature map.\n",
        "\n",
        "![Convolutional Layer](https://editor.analyticsvidhya.com/uploads/578272021-07-20%2023_09_31-ML%20Practicum_%20Image%20Classification%20%C2%A0_%C2%A0%20Google%20Developers.png)\n",
        "\n",
        "## 2. Pooling Layer\n",
        "\n",
        "Pooling Layers are utilized to reduce the dimensionality of feature maps. Here, a 2x2 max-pooling layer is used. As the window traverses the image, it selects the maximum value within the window:\n",
        "\n",
        "![Maxpool Layer](https://editor.analyticsvidhya.com/uploads/66402maxpool_animation.gif)\n",
        "\n",
        "Following the max-pooling operation, the input's dimension, originally 4x4, is downsized to 2x2. This dimension reduction is essential for managing computational complexity and enhancing feature recognition.\n",
        "\n",
        "## 3. Fully Connected Layer\n",
        "\n",
        "The Fully Connected Layer serves as the final section of the CNN architecture. It receives the rich features extracted from the CIFAR-10 dataset using convolutional filters. These features are then forwarded through the network, ultimately reaching the output layer. In the output layer, the model predicts the probability of the input image belonging to one of the ten predefined classes. The final predicted output corresponds to the class with the highest probability according to the model's prediction.\n",
        "\n",
        "In summary, this explanation provides an overview of the primary components of a CNN architecture when applied to the CIFAR-10 dataset. Convolutional Layers extract features, Pooling Layers reduce dimensionality, and Fully Connected Layers make the final class predictions. This understanding forms the foundation for implementing the CNN architecture in code.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0fOePkVZDQ-"
      },
      "id": "Y0fOePkVZDQ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f0b4db",
      "metadata": {
        "id": "a8f0b4db"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the ConvNet using nn.Sequential\n",
        "conv_net = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 5),  # Convolutional layer 1: 3 input channels, 6 filters, 5x5 filter size\n",
        "    nn.ReLU(),          # ReLU activation function\n",
        "    nn.MaxPool2d(2, 2), # Max pooling layer: size 2x2, stride 2\n",
        "\n",
        "    nn.Conv2d(6, 16, 5), # Convolutional layer 2: 6 input channels, 16 filters, 5x5 filter size\n",
        "    nn.ReLU(),          # ReLU activation function\n",
        "    nn.MaxPool2d(2, 2), # Max pooling layer: size 2x2, stride 2\n",
        "\n",
        "    nn.Flatten(),       # Flatten the output for fully connected layers\n",
        "\n",
        "    nn.Linear(16*5*5, 120), # Fully connected layer 1\n",
        "    nn.ReLU(),              # ReLU activation function\n",
        "\n",
        "    nn.Linear(120, 84),     # Fully connected layer 2\n",
        "    nn.ReLU(),              # ReLU activation function\n",
        "\n",
        "    nn.Linear(84, 10)       # Fully connected layer 3 (output layer)\n",
        ")\n",
        "\n",
        "# Define the device (CPU or GPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of the ConvNet model and move it to the specified device\n",
        "model = conv_net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "numChannels: The number of channels in the input images (1 for grayscale or 3 for RGB)"
      ],
      "metadata": {
        "id": "qLBonSZ1Yza3"
      },
      "id": "qLBonSZ1Yza3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa5cea5",
      "metadata": {
        "id": "1fa5cea5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss already includes SoftMax\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b1b6c4",
      "metadata": {
        "id": "c2b1b6c4",
        "outputId": "e379084a-89cc-40f2-b9f4-28302eae4e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [2000/3125], Loss: 2.2935\n",
            "Epoch [2/4], Step [2000/3125], Loss: 1.6798\n",
            "Epoch [3/4], Step [2000/3125], Loss: 1.5854\n",
            "Epoch [4/4], Step [2000/3125], Loss: 1.5661\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad() #with zero_grad() we ensure that the gradients are properly reset to zero at the start of each iteration\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #wandb.log({\"loss\": loss})  uncomment this line if you want to send data to weights and biases interface\n",
        "\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "            # Calculate average accuracy for every 2000 steps\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for images, labels in test_loader:\n",
        "                    images = images.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "                accuracy = correct / total\n",
        "                #wandb.log({\"accuracy\": accuracy}) uncomment this line if you want to send data to weights and biases interface\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(labels.size(0)):  # Use labels.size(0) to get the batch size\n",
        "            label = labels[i].item()  # Extract the actual label value using .item()\n",
        "            pred = predicted[i].item()  # Extract the predicted label value using .item()\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        if n_class_samples[i] == 0:\n",
        "            acc = 0.0  # To avoid division by zero\n",
        "        else:\n",
        "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ3uBVHTPbJx",
        "outputId": "be1fbe21-4060-49d2-9e3e-0eb5b7365cad"
      },
      "id": "mJ3uBVHTPbJx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 44.76 %\n",
            "Accuracy of plane: 47.8 %\n",
            "Accuracy of car: 59.4 %\n",
            "Accuracy of bird: 13.4 %\n",
            "Accuracy of cat: 27.3 %\n",
            "Accuracy of deer: 46.1 %\n",
            "Accuracy of dog: 33.5 %\n",
            "Accuracy of frog: 57.1 %\n",
            "Accuracy of horse: 50.8 %\n",
            "Accuracy of ship: 54.8 %\n",
            "Accuracy of truck: 57.4 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}